{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1a3034",
   "metadata": {},
   "source": [
    "# Indus Valley Script- Text Analysis for Decipherment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b03a1",
   "metadata": {},
   "source": [
    "## Input data processing and data cleanup\n",
    "\n",
    "Dataset was created as a csv file from ICIT web site from raw html files of ICIT code for each for the Text\n",
    "Data labels were changes and a linearized copy of the original text was added\n",
    "\n",
    "### Input:\n",
    "icit_text_text_corpus.csv and icit_sign_corpus.csv are the input csv\n",
    "\n",
    "### Output:\n",
    "Various Pickled dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5b49",
   "metadata": {},
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install ipywidgets\n",
    "!pip install -U dill\n",
    "!pip3 install requests\n",
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa43bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import collections\n",
    "import random\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057c8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7004991",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_duplicate_texts = True\n",
    "\n",
    "# Set the filters on data here\n",
    "filter_by_site = False\n",
    "filter_by_keywords = False\n",
    "filter_by_text_length= False\n",
    "\n",
    "#site = 'Mohenjo-daro'\n",
    "#site = 'Harappa'\n",
    "#site = 'Dholavira'\n",
    "#site = 'Rakhigarhi'\n",
    "#keyword = \"Bull\"\n",
    "#keyword = \"Gaur\"\n",
    "\n",
    "min_text_length=1\n",
    "max_text_length=50\n",
    "\n",
    "num_rows_text_corpus= 4999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54305a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the signs\n",
    "orig_sign_df=pd.read_csv('../../IndusCorpusUtils/data/icit_corpus/icit_sign_corpus.csv',dtype=str)\n",
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "orig_sign_df\n",
    "\n",
    "\"\"\"Pickle it\"\"\"\n",
    "orig_sign_df.to_pickle('pickle/orig_sign_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bd36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Text Corpus\n",
    "orig_df=pd.read_csv('../../IndusCorpusUtils/data/icit_corpus/icit_text_text_corpus.csv',dtype=str, nrows=num_rows_text_corpus)\n",
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b92e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has  4999  rows\n",
      "     icit_id        site keywords text_class lines direction       text signs  \\\n",
      "0          1  Alamgirpur      NaN         SS     1       L/R  +410-017+     2   \n",
      "1          2  Alamgirpur      NaN         SS     1       L/R  +410-017+     2   \n",
      "2          3  Alamgirpur      NaN         SC     1       L/R  +405-017+     2   \n",
      "3          4   Allahdino      NaN         ??     1       NaN  +220-000+     1   \n",
      "4          5   Allahdino     Bull         UC     1       R/L  +740-235+     2   \n",
      "...      ...         ...      ...        ...   ...       ...        ...   ...   \n",
      "4994    4064     Harappa      NaN         UC     1       NaN      +000[     0   \n",
      "4995    4065     Harappa      NaN         VN     1       R/L  ]700-032[     2   \n",
      "4996    4065     Harappa      NaN         UC     1       R/L  ]000-000[     0   \n",
      "4997    4066     Harappa      NaN         UC     1       R/L  +368-000+     1   \n",
      "4998    4066     Harappa      NaN         VN     1       R/L  +700-033+     2   \n",
      "\n",
      "     complete    alignment  sign height text_images linearized_text  \\\n",
      "0           Y    Unordered      Unequal         NaN         410 017   \n",
      "1           Y          NaN          NaN         NaN         410 017   \n",
      "2           Y          NaN          NaN         NaN         405 017   \n",
      "3           N          NaN          NaN         NaN         220 000   \n",
      "4           Y          NaN          NaN         NaN         740 235   \n",
      "...       ...          ...          ...         ...             ...   \n",
      "4994        N  Indefinable  Indefinable         NaN            000[   \n",
      "4995        ?          NaN          NaN         NaN        700 032[   \n",
      "4996        N          NaN          NaN         NaN        000 000[   \n",
      "4997        N          NaN          NaN         NaN         368 000   \n",
      "4998        Y          NaN          NaN         NaN         700 033   \n",
      "\n",
      "     l_to_r_text r_to_l_text reversed_text  text_length  \n",
      "0        410 017     017 410       017 410          2.0  \n",
      "1        410 017     017 410       017 410          2.0  \n",
      "2        405 017     017 405       017 405          2.0  \n",
      "3        000 220     000 220       220 000          2.0  \n",
      "4        235 740     740 235       740 235          2.0  \n",
      "...          ...         ...           ...          ...  \n",
      "4994        000[        000[          000[          1.0  \n",
      "4995    032[ 700    700 032[      700 032[          3.0  \n",
      "4996    000[ 000    000 000[      000 000[          3.0  \n",
      "4997     000 368     368 000       368 000          2.0  \n",
      "4998     033 700     700 033       700 033          2.0  \n",
      "\n",
      "[4999 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reverse text and add that as a new column\n",
    "# Add text length as a column\n",
    "list_reversed_text = []\n",
    "for text in orig_df[orig_df.l_to_r_text!=''].l_to_r_text:\n",
    "    # Tokenize to words\n",
    "    # first split the string into chars\n",
    "    chars = text.split(' ')\n",
    "    length = len(chars)\n",
    "    # then reverse the split string list and join with a space\n",
    "    reversed_text = ' '.join(reversed(chars))\n",
    "    list_reversed_text.append(reversed_text)\n",
    "    \n",
    "orig_df['reversed_text']= list_reversed_text #same as r_to_l text\n",
    "orig_df['text_length']= orig_df['l_to_r_text'].str.len().div(3).round()\n",
    "\n",
    "print(\"Dataframe has \", len(orig_df.index), \" rows\")\n",
    "\n",
    "print(orig_df)\n",
    "\n",
    "\"\"\"Pickle it\"\"\"\n",
    "orig_df.to_pickle('pickle/upd_orig_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3353f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.copy()\n",
    "\n",
    "if(filter_by_site==True):\n",
    "    #keep only the values that matches the provided site\n",
    "    df = df[df['site'].str.contains(site) == True] \n",
    "    print(\"After filtering by site \", site, \" it has \", len(df.index), \" rows\")\n",
    "\n",
    "if(filter_by_keywords==True):\n",
    "     #keep only the values that matches the provided keyword\n",
    "    df = df[df['keywords'].str.contains(keyword) == True] \n",
    "    print(\"After filtering by keywords \", keyword, \" it has \", len(df.index), \" rows\")\n",
    "\n",
    "if(filter_by_text_length==True):\n",
    "    df = df[(df['text_length'] > min_text_length) & (df['text_length']< max_text_length)]\n",
    "    print(\"After filtering by text_length \",  \" it has \", len(df.index), \" rows\")\n",
    "\n",
    "\n",
    "df_filtered = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8749f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing unclear texts, we have  3945  rows\n",
      "After removing duplicate texts, we have  2856  rows\n",
      "After removing multi-line text, we have  2778  rows\n",
      "After keeping only text with known direction, we have  2646  rows\n",
      "After keeping only text without multipart, we have  2223  rows\n"
     ]
    }
   ],
   "source": [
    "# Retain texts that are only wanted\n",
    "\n",
    "#remove the values where the text is unclear\n",
    "df = df[df['l_to_r_text'].str.contains('000') == False] \n",
    "\n",
    "print(\"After removing unclear texts, we have \", len(df.index), \" rows\")\n",
    "\n",
    "if(drop_duplicate_texts):\n",
    "    #Remove out duplicate inplace\n",
    "    \n",
    "    # We will consider a text duplicate only of the keywords(pics) are different\n",
    "    # In that case we will retain the first occurance of it\n",
    "    df = df.drop_duplicates(subset =[\"text\", \"keywords\", \"site\"], inplace = False, keep = \"first\")\n",
    "    print(\"After removing duplicate texts, we have \", len(df.index), \" rows\")\n",
    "\n",
    "\n",
    "#keep only the values that does not have multi-line text\n",
    "df = df[df['text'].str.contains('/') == False] \n",
    "\n",
    "print(\"After removing multi-line text, we have \", len(df.index), \" rows\")\n",
    "\n",
    "\n",
    "#Single sign ones don't have direction and won't have /, wo we need to explicitly include it\n",
    "#Btw standardized_text is Left to right as in English\n",
    "df = df[(df['direction'].str.contains('/') == True) | (df['text_length'] ==1)] \n",
    "\n",
    "print(\"After keeping only text with known direction, we have \", len(df.index), \" rows\")\n",
    "\n",
    "#Remove Multipart texts that have [ or ]\n",
    "df = df[df['text'].str.contains(\"\\[\") == False] \n",
    "df = df[df['text'].str.contains(\"\\]\") == False] \n",
    "\n",
    "print(\"After keeping only text without multipart, we have \", len(df.index), \" rows\")\n",
    "\n",
    "\"\"\"Pickle it\"\"\"\n",
    "df.to_pickle('pickle/clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56653c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L/R texts:  97\n",
      "R/L texts:  1985\n"
     ]
    }
   ],
   "source": [
    "# of those whose direction is know print out L/R and L/R text count\n",
    "df_l_r = df[df['direction'].str.contains('L/R') == True] \n",
    "\n",
    "print(\"L/R texts: \", len(df_l_r.index))\n",
    "\n",
    "df_r_l = df[df['direction'].str.contains('R/L') == True]\n",
    "\n",
    "print(\"R/L texts: \", len(df_r_l.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4def047",
   "metadata": {},
   "source": [
    "# Unclear Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3c88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1054  rows of unclear texts\n",
      "After removing duplicate texts, we have  763  rows\n",
      "After removing multi-line text, we have  728  rows\n",
      "After keeping only text with known direction, we have  586  rows\n",
      "After keeping only text without multipart, we have  291  rows\n"
     ]
    }
   ],
   "source": [
    "#Keep the items with unclear text in another dataframe\n",
    "df_unclear = df_filtered[df_filtered['l_to_r_text'].str.contains('000') == True]\n",
    "\n",
    "print(\"We have\", len(df_unclear.index), \" rows of unclear texts\")\n",
    "\n",
    "if(drop_duplicate_texts):\n",
    "    #Remove out duplicate inplace\n",
    "    # TBD How can unclear text be duplicates\n",
    "    df_unclear  =df_unclear.drop_duplicates(subset =\"text\",\n",
    "                         keep = False, inplace = False)\n",
    "\n",
    "    print(\"After removing duplicate texts, we have \", len(df_unclear.index), \" rows\")\n",
    "\n",
    "#keep only the values that does not have multi-line text\n",
    "df_unclear= df_unclear[df_unclear['text'].str.contains('/') == False] \n",
    "\n",
    "print(\"After removing multi-line text, we have \", len(df_unclear.index), \" rows\")\n",
    "\n",
    "\n",
    "#Single sign ones don't have direction and won't have /, wo we need to explicitly include it\n",
    "#Btw standardized_text is Left to right as in English\n",
    "#df = df[df['direction'].str.contains('/') == True] \n",
    "df_unclear= df_unclear[(df_unclear['direction'].str.contains('/') == True) | (df_unclear['text_length'] ==1)] \n",
    "\n",
    "print(\"After keeping only text with known direction, we have \", len(df_unclear.index), \" rows\")\n",
    "\n",
    "#Remove Multipart texts that have [ or ]\n",
    "df_unclear= df_unclear[df_unclear['text'].str.contains(\"\\[\") == False] \n",
    "df_unclear= df_unclear[df_unclear['text'].str.contains(\"\\]\") == False] \n",
    "\n",
    "print(\"After keeping only text without multipart, we have \", len(df_unclear.index), \" rows\")\n",
    "\n",
    "#Note: Lot of the text with unclear text have direction empty\n",
    "\n",
    "\"\"\"Pickle it\"\"\"\n",
    "df_unclear.to_pickle('pickle/unclear_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd15783",
   "metadata": {},
   "source": [
    "# Multi Line Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93e3802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has  4999  rows\n",
      "After removing unclear texts, we have  3945  rows\n",
      "After removing duplicate texts, we have  2130  rows\n",
      "We have 77  rows of multi line texts\n",
      "69                      +032-031/151-740-240-235+\n",
      "71              +032-031/850-032-530-740-741-456+\n",
      "72                          +032-031/740-791-713+\n",
      "74                              +032/226-032-817+\n",
      "80                          +740-636-240/002-817+\n",
      "                          ...                    \n",
      "4386                    +621/090-740-231-560-534+\n",
      "4402                +790/740-100-415-740-257-840+\n",
      "4705                        +740-900-003/741-002+\n",
      "4729                                    +840/790+\n",
      "4752    +605-740-142-067/002-374-310-350-495-834+\n",
      "Name: text, Length: 77, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Keep the text that are multiline (has ''/'') in another dataframe\n",
    "\n",
    "print(\"Dataframe has \", len(df_filtered.index), \" rows\")\n",
    "\n",
    "#remove the values where the text is unclear\n",
    "df_multi_line = df_filtered[df_filtered['l_to_r_text'].str.contains('000') == False] \n",
    "\n",
    "print(\"After removing unclear texts, we have \", len(df_multi_line.index), \" rows\")\n",
    "\n",
    "if(drop_duplicate_texts):\n",
    "    #Remove out duplicate inplace\n",
    "    df_multi_line = df_multi_line.drop_duplicates(subset =\"text\",\n",
    "                         keep = False, inplace = False)\n",
    "\n",
    "    print(\"After removing duplicate texts, we have \", len(df_multi_line.index), \" rows\")\n",
    "\n",
    "\n",
    "#keep only the values that has multi-line text\n",
    "df_multi_line = df_multi_line[df_multi_line['text'].str.contains('/') == True] \n",
    "\n",
    "print(\"We have\", len(df_multi_line.index), \" rows of multi line texts\")\n",
    "print(df_multi_line.text)\n",
    "\n",
    "df_multi_line.to_csv('multi_line_texts.csv')\n",
    "\n",
    "\"\"\"Pickle it\"\"\"\n",
    "df_multi_line.to_pickle('pickle/multi_line_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de932926",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e209fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will keep All data, Train data and Test data'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We will keep All data, Train data and Test data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c6896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2223,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = df['site'].values\n",
    "y_all.shape\n",
    "\n",
    "y=df['site'].values\n",
    "y.shape\n",
    "\n",
    "# y axis is still the same\n",
    "y_all_rev=df['site'].values\n",
    "y_all_rev.shape\n",
    "\n",
    "y_rev=df['site'].values\n",
    "y_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639a81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2223,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_all = np.asarray(df[['l_to_r_text', 'direction']])\n",
    "x_all.shape\n",
    "\n",
    "x = np.asarray(df[['l_to_r_text', 'direction']])\n",
    "x.shape\n",
    "\n",
    "x_all_rev=df['reversed_text'].values\n",
    "x_all_rev.shape\n",
    "\n",
    "x_rev=df['reversed_text'].values\n",
    "x_rev.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e872a",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8ac3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f0e4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test,y_train,y_test)=train_test_split(x,y,test_size=0.2, random_state=43)\n",
    "\n",
    "(x_rev_train,x_rev_test,y_rev_train,y_rev_test)=train_test_split(x_rev,y_rev,test_size=0.1, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5885aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All data, fwd and reverse\n",
    "df_all_x=pd.DataFrame(x_all)\n",
    "df_all_x=df_all_x.rename(columns={0:'l_to_r_text'})\n",
    "df_all_x=df_all_x.rename(columns={1:'direction'})\n",
    "\n",
    "df_all_y=pd.DataFrame(y_all)\n",
    "df_all_y=df_all_y.rename(columns={0:'site'})\n",
    "\n",
    "#rev\n",
    "df_all_x_rev=pd.DataFrame(x_all_rev)\n",
    "df_all_x_rev=df_all_x_rev.rename(columns={0:'reversed_text'})\n",
    "\n",
    "df_all_y_rev=pd.DataFrame(y_all_rev)\n",
    "df_all_y_rev=df_all_y_rev.rename(columns={0:'site'})\n",
    "\n",
    "#Train data, fwd and reverse\n",
    "df_train_x=pd.DataFrame(x_train)\n",
    "df_train_x=df_train_x.rename(columns={0:'l_to_r_text'})\n",
    "df_train_x=df_train_x.rename(columns={1:'direction'})\n",
    "\n",
    "df_train_y=pd.DataFrame(y_train)\n",
    "df_train_y=df_train_y.rename(columns={0:'site'})\n",
    "\n",
    "#rev\n",
    "df_train_x_rev=pd.DataFrame(x_rev_train)\n",
    "df_train_x_rev=df_train_x_rev.rename(columns={0:'reversed_text'})\n",
    "\n",
    "\n",
    "df_train_y_rev=pd.DataFrame(y_rev_train)\n",
    "df_train_y_rev=df_train_y_rev.rename(columns={0:'site'})\n",
    "\n",
    "#Test data, fwd and reverse\n",
    "df_test_x=pd.DataFrame(x_test)\n",
    "df_test_x=df_test_x.rename(columns={0:'l_to_r_text'})\n",
    "df_test_x=df_test_x.rename(columns={1:'direction'})\n",
    "\n",
    "df_test_y=pd.DataFrame(y_test)\n",
    "df_test_y=df_test_y.rename(columns={0:'site'})\n",
    "\n",
    "#rev\n",
    "df_test_x_rev=pd.DataFrame(x_rev_test)\n",
    "df_test_x_rev=df_test_x_rev.rename(columns={0:'reversed_text'})\n",
    "\n",
    "df_test_y_rev=pd.DataFrame(y_rev_test)\n",
    "df_test_y_rev=df_test_y_rev.rename(columns={0:'site'})\n",
    "\n",
    "\"\"\"Pickle the data\"\"\"\n",
    "df_all_x.to_pickle('pickle/all_x.pkl')\n",
    "df_all_y.to_pickle('pickle/all_y.pkl')\n",
    "\n",
    "df_all_x_rev.to_pickle('pickle/all_x_rev.pkl')\n",
    "df_all_y_rev.to_pickle('pickle/all_y_rev.pkl')\n",
    "\n",
    "\n",
    "df_train_x.to_pickle('pickle/train_x.pkl')\n",
    "df_train_y.to_pickle('pickle/train_y.pkl')\n",
    "\n",
    "df_train_x_rev.to_pickle('pickle/train_x_rev.pkl')\n",
    "df_train_y_rev.to_pickle('pickle/train_y_rev.pkl')\n",
    "\n",
    "df_test_x.to_pickle('pickle/test_x.pkl')\n",
    "df_test_y.to_pickle('pickle/test_y.pkl')\n",
    "\n",
    "df_test_x_rev.to_pickle('pickle/test_x_rev.pkl')\n",
    "df_test_y_rev.to_pickle('pickle/test_y_rev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf71ca4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       l_to_r_text direction        site\n",
      "0          410 017       L/R  Alamgirpur\n",
      "1          405 017       L/R  Alamgirpur\n",
      "2          235 740       R/L   Allahdino\n",
      "3      590 390 740       R/L   Allahdino\n",
      "4  033 125 390 368       R/L   Allahdino\n",
      "                   l_to_r_text direction          site\n",
      "0                          013        NR        Lothal\n",
      "1                      700 034       R/L       Harappa\n",
      "2                  590 407 740       R/L         Hulas\n",
      "3      820 002 806 590 405 740       R/L  Mohenjo-daro\n",
      "4  140 920 484 337 503 456 400       R/L  Mohenjo-daro\n",
      "                   l_to_r_text direction          site\n",
      "0                      003 390       R/L  Mohenjo-daro\n",
      "1                  235 240 520       R/L       Harappa\n",
      "2  861 002 003 220 590 405 740       R/L       Harappa\n",
      "3                          820        NR       Harappa\n",
      "4  140 287 002 415 220 879 740       R/L  Mohenjo-daro\n",
      "     reversed_text        site\n",
      "0          017 410  Alamgirpur\n",
      "1          017 405  Alamgirpur\n",
      "2          740 235   Allahdino\n",
      "3      740 390 590   Allahdino\n",
      "4  368 390 125 033   Allahdino\n",
      "                                       reversed_text          site\n",
      "0                        740 100 176 840 013 002 817  Mohenjo-daro\n",
      "1                                    875 700 384 125  Mohenjo-daro\n",
      "2  740 877 032 033 705 231 235 002 861 360 520 91...  Mohenjo-daro\n",
      "3                                            002 818  Mohenjo-daro\n",
      "4                        740 798 460 350 741 590 032       Harappa\n",
      "                 reversed_text          site\n",
      "0                      390 003  Mohenjo-daro\n",
      "1                  520 240 235       Harappa\n",
      "2  740 405 590 220 003 002 861       Harappa\n",
      "3                          820       Harappa\n",
      "4  740 879 220 415 002 287 140  Mohenjo-daro\n"
     ]
    }
   ],
   "source": [
    "df_all=pd.concat([df_all_x,df_all_y],axis=1)\n",
    "print(df_all.head())\n",
    "\n",
    "df_train=pd.concat([df_train_x,df_train_y],axis=1)\n",
    "print(df_train.head())\n",
    "\n",
    "df_test=pd.concat([df_test_x,df_test_y],axis=1)\n",
    "print(df_test.head())\n",
    "\n",
    "df_all_rev=pd.concat([df_all_x_rev,df_all_y_rev],axis=1)\n",
    "print(df_all_rev.head())\n",
    "\n",
    "df_train_rev=pd.concat([df_train_x_rev,df_train_y_rev],axis=1)\n",
    "print(df_train_rev.head())\n",
    "\n",
    "df_test_rev=pd.concat([df_test_x_rev,df_test_y_rev],axis=1)\n",
    "print(df_test_rev.head())\n",
    "\n",
    "\"\"\"Pickle all the dataframes we need\"\"\"\n",
    "df_all.to_pickle('pickle/all_df.pkl')\n",
    "df_train.to_pickle('pickle/train_df.pkl')\n",
    "df_test.to_pickle('pickle/test_df.pkl')\n",
    "df_all_rev.to_pickle('pickle/all_rev_df.pkl')\n",
    "df_train_rev.to_pickle('pickle/train_rev_df.pkl')\n",
    "df_test_rev.to_pickle('pickle/test_rev_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
