{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62002e2",
   "metadata": {},
   "source": [
    "# Indus Valley Script- Text Analysis for Decipherment\n",
    "# Anomalous Text Analysis\n",
    "\n",
    "This file is used to identify Anamalous texts using the language models\n",
    "\n",
    "Dataset was created as a csv file from ICIT web site from raw html files of ICIT code for each for the Text\n",
    "Data labels were changes and a linearized copy of the original text was added\n",
    "\n",
    "### Input:\n",
    "Pickled data file from Language Models and Indus Core and Non Core region data\n",
    "\n",
    "### Output:\n",
    "Test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!python3 -m pip install matplotlib\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install nltk\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install -U dill\n",
    "!pip3 install requests\n",
    "!pip3 install -U spacy\n",
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e991f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import collections\n",
    "import random\n",
    "import traceback\n",
    "import pickle\n",
    "import copy\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#plt.style.use(style='seaborn')\n",
    "#plt.style.use(\"seaborn-v0_8\")\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82025698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from collections import defaultdict\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.lm.models import Lidstone\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47b6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8\n",
    "seed = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb3d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Train size: 1770\n",
      "Core Test size: 443\n",
      "Indus Non Core text df: \n",
      "      icit_id                site             keywords text_class lines  \\\n",
      "4782    3884           Tell Umma                Bull1         PP     1   \n",
      "4796    3897                  Ur                 Gaur         SC     1   \n",
      "4797    3898                  Ur                 Gaur         UC     1   \n",
      "15        16          Altyn Depe                  NaN         UC     1   \n",
      "4780    3882                Susa                 Gaur         LC     1   \n",
      "2928    2153            Luristan                 Gaur         SC     1   \n",
      "2721    1971                Kish             Bull1:II         LP     1   \n",
      "4758    3863  Qala'at al-Bahrain                 Gaur         SC     1   \n",
      "4760    3865      Ra's al-Junayz                  NaN         SS     1   \n",
      "161      160               Hajar  Gaur:Scene, 2 Goats         SS     1   \n",
      "\n",
      "     direction                               text signs complete  \\\n",
      "4782       R/L              +390-004-002-705-127+     5        Y   \n",
      "4796       R/L              +093-340-924-220-528+     5        Y   \n",
      "4797       R/L          +002-004-328-001-803-415+     6        Y   \n",
      "15         L/R                          +390-415+     2        Y   \n",
      "4780       R/L  +416-150-002-055-031-319-001-924+     8        Y   \n",
      "2928       R/L                  +831-413-840-091+     4        Y   \n",
      "2721       R/L  +740-390-590-220-003-060-840-416+     8        Y   \n",
      "4758       L/R              +160-090-055-060-190+     5        Y   \n",
      "4760       R/L                          +405-004+     2        Y   \n",
      "161        R/L                          +617-117+     2        Y   \n",
      "\n",
      "          alignment sign height text_images                  linearized_text  \\\n",
      "4782            NaN         NaN         NaN              390 004 002 705 127   \n",
      "4796  Partly linear     Unequal         NaN              093 340 924 220 528   \n",
      "4797  Partly linear     Unequal         NaN          002 004 328 001 803 415   \n",
      "15           Linear       Equal         NaN                          390 415   \n",
      "4780  Partly linear       Equal         NaN  416 150 002 055 031 319 001 924   \n",
      "2928         Linear     Unequal         NaN                  831 413 840 091   \n",
      "2721         Linear       Equal         NaN  740 390 590 220 003 060 840 416   \n",
      "4758      Unordered     Unequal         NaN              160 090 055 060 190   \n",
      "4760            NaN         NaN         NaN                          405 004   \n",
      "161             NaN         NaN         NaN                          617 117   \n",
      "\n",
      "                          l_to_r_text                      r_to_l_text  \\\n",
      "4782              127 705 002 004 390              390 004 002 705 127   \n",
      "4796              528 220 924 340 093              093 340 924 220 528   \n",
      "4797          415 803 001 328 004 002          002 004 328 001 803 415   \n",
      "15                            390 415                          415 390   \n",
      "4780  924 001 319 031 055 002 150 416  416 150 002 055 031 319 001 924   \n",
      "2928                  091 840 413 831                  831 413 840 091   \n",
      "2721  416 840 060 003 220 590 390 740  740 390 590 220 003 060 840 416   \n",
      "4758              160 090 055 060 190              190 060 055 090 160   \n",
      "4760                          004 405                          405 004   \n",
      "161                           117 617                          617 117   \n",
      "\n",
      "                        reversed_text  text_length  \n",
      "4782              390 004 002 705 127          6.0  \n",
      "4796              093 340 924 220 528          6.0  \n",
      "4797          002 004 328 001 803 415          8.0  \n",
      "15                            415 390          2.0  \n",
      "4780  416 150 002 055 031 319 001 924         10.0  \n",
      "2928                  831 413 840 091          5.0  \n",
      "2721  740 390 590 220 003 060 840 416         10.0  \n",
      "4758              190 060 055 090 160          6.0  \n",
      "4760                          405 004          2.0  \n",
      "161                           617 117          2.0  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\" Unpickle some of the Indus Core  and Non Core Region dataframes needed \"\"\"\n",
    "\n",
    "df_core = pd.read_pickle('pickle/core_df.pkl')\n",
    "df_core_train_x = pd.read_pickle('pickle/core_train_x.pkl')\n",
    "df_core_train_y = pd.read_pickle('pickle/core_train_y.pkl')\n",
    "df_core_train = pd.read_pickle('pickle/core_train_df.pkl')\n",
    "\n",
    "\n",
    "df_core_rev = pd.read_pickle('pickle/core_rev_df.pkl')\n",
    "df_core_train_x_rev = pd.read_pickle('pickle/core_train_x_rev.pkl')\n",
    "df_core_train_y_rev = pd.read_pickle('pickle/core_train_y_rev.pkl')\n",
    "df_core_train_rev = pd.read_pickle('pickle/core_train_rev_df.pkl')\n",
    "\n",
    "df_core_test_x = pd.read_pickle('pickle/core_test_x.pkl')\n",
    "df_core_test_y= pd.read_pickle('pickle/core_test_y.pkl')\n",
    "df_core_test = pd.read_pickle('pickle/core_test_df.pkl')\n",
    "\n",
    "\n",
    "print(\"Core Train size:\", len(df_core_train_x))\n",
    "print(\"Core Test size:\", len(df_core_test_x))\n",
    "\n",
    "df_core_test_x_rev = pd.read_pickle('pickle/core_test_x_rev.pkl')\n",
    "df_core_test_y_rev = pd.read_pickle('pickle/core_test_y_rev.pkl')\n",
    "df_core_test_rev = pd.read_pickle('pickle/core_test_rev_df.pkl')\n",
    "\n",
    "\n",
    "\"\"\" Indus Non Core region texts\"\"\"\n",
    "df_indus_non_core_region = pd.read_pickle('pickle/non_core_df.pkl')\n",
    "print(\"Indus Non Core text df: \\n\", df_indus_non_core_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53aef0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to Restore the models trained by Indus Core train data\"\"\"\n",
    "def restore_model(this_model_name, this_model_order, direction, data_type):\n",
    "\n",
    "    try:\n",
    "        \"\"\"Unpickle the model\"\"\"\n",
    "\n",
    "        file_name = \"pickle/\"+ this_model_name + \"_\" + direction + \"_\" + data_type + \"_\"+ str(this_model_order) + \".pkl\"\n",
    "        print(\"Unpickling Model from \", file_name)\n",
    "        with open(file_name , 'rb') as f:\n",
    "            this_model = pickle.load(f)\n",
    "\n",
    "        print(\"Unpicked Model Check:\",this_model,\"Order:\", this_model.order, this_model.vocab)\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e.__class__, \"restore_model\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return copy.deepcopy(this_model)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1425788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling Model from  pickle/MLE_fwd_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_fwd_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_fwd_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_fwd_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_fwd_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_fwd_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_fwd_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/MLE_rev_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.MLE object at 0x1059f7f40> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_fwd_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/KneserNeyInterpolated_rev_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.KneserNeyInterpolated object at 0x15d2d0fa0> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_rev_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_rev_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_rev_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_rev_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_rev_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_rev_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Laplace_fwd_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling Model from  pickle/Laplace_rev_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Laplace object at 0x1059f7f40> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_fwd_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/Lidstone_rev_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.Lidstone object at 0x15d2d0fa0> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_7.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_fwd_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/StupidBackoff_rev_core_train_8.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.StupidBackoff object at 0x1059f7f40> Order: 8 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_fwd_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_rev_core_train_2.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_fwd_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_rev_core_train_3.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_fwd_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_rev_core_train_4.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_fwd_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_rev_core_train_5.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_fwd_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n",
      "Unpickling Model from  pickle/WittenBellInterpolated_rev_core_train_6.pkl\n",
      "Unpicked Model Check: <nltk.lm.models.WittenBellInterpolated object at 0x15d2d0fa0> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 547 items>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         model_core_fwd[index][model_order] \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(model_order)\n\u001b[1;32m     13\u001b[0m         model_core_fwd[index][model_order] \u001b[38;5;241m=\u001b[39m restore_model(model_name, model_order,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfwd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m         model_core_rev[index][model_order] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_order\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcore_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m#print(\"---> Setting \", model_name, index, model_order, \":\", model_core_fwd[index][model_order] )\u001b[39;00m\n\u001b[1;32m     17\u001b[0m verbose_debug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 17\u001b[0m, in \u001b[0;36mrestore_model\u001b[0;34m(this_model_name, this_model_order, direction, data_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestore_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:265\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n\u001b[1;32m    264\u001b[0m     args \u001b[38;5;241m=\u001b[39m (deepcopy(arg, memo) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[0;32m--> 265\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m    267\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/probability.py:102\u001b[0m, in \u001b[0;36mFreqDist.__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Construct a new frequency distribution.  If ``samples`` is\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    given, then the frequency distribution will be initialized\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    :type samples: Sequence\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mCounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Cached number of samples in this FreqDist\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/collections/__init__.py:577\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 577\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/probability.py:140\u001b[0m, in \u001b[0;36mFreqDist.update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mOverride ``Counter.update()`` to invalidate the cached N\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/collections/__init__.py:661\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# The regular dict.update() operation makes no sense here because the\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# replace behavior results in the some of original untouched counts\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# being mixed-in with all of the other counts for a mismash that\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# doesn't have a straight-forward interpretation in most counting\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# contexts.  Instead, we implement straight-addition.  Both the inputs\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# and outputs are allowed to contain zero and negative counts.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_collections_abc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMapping\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    663\u001b[0m             self_get \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/abc.py:119\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_abc_instancecheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Restore the models trained by Indus Core train data\"\"\"\n",
    "\n",
    "model_name_list = [\"MLE\", \"KneserNeyInterpolated\", \"Laplace\", \"Lidstone\",\"StupidBackoff\", \"WittenBellInterpolated\"]\n",
    "model_core_fwd = [[0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]]\n",
    "model_core_rev = [[0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "#print(model_core_fwd)\n",
    "\n",
    "for model_name in model_name_list:\n",
    "    index = model_name_list.index(model_name)\n",
    "    for model_order in range(2, 9):\n",
    "        model_core_fwd[index][model_order] = model_name + \":\" + str(index) + \":\" + str(model_order)\n",
    "        model_core_fwd[index][model_order] = restore_model(model_name, model_order,\"fwd\",\"core_train\")\n",
    "        model_core_rev[index][model_order] = restore_model(model_name, model_order,\"rev\",\"core_train\")\n",
    "        #print(\"---> Setting \", model_name, index, model_order, \":\", model_core_fwd[index][model_order] )\n",
    "\n",
    "verbose_debug = False\n",
    "if(verbose_debug):\n",
    "    for model_name in model_name_list:\n",
    "        index = model_name_list.index(model_name)\n",
    "        for model_order in range(2, 9):\n",
    "            print(\"**\", index, model_order, model_core_fwd[index][model_order],  model_core_fwd[index][model_order].order)\n",
    "            #print(\"**\", index, model_order, model_core_fwd[index][model_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unpickle the Lidstone Septagram model for All Indus Core region data\"\"\"\n",
    "\n",
    "file_name = \"pickle/Lidstone_fwd_all_core_train_7.pkl\"\n",
    "print(\"Unpickling Model from \", file_name)\n",
    "with open(file_name , 'rb') as f:\n",
    "    model_all_core_sep_fwd_lid = pickle.load(f)\n",
    "    \n",
    "file_name = \"pickle/Lidstone_rev_all_core_train_7.pkl\"\n",
    "print(\"Unpickling Model from \", file_name)\n",
    "with open(file_name , 'rb') as f:\n",
    "    model_all_core_sep_rev_lid = pickle.load(f)\n",
    "    \n",
    "print(\"Unpicked Model Check:\",model_all_core_sep_fwd_lid,\"Order:\", model_all_core_sep_fwd_lid.order, model_all_core_sep_fwd_lid.vocab)\n",
    "print(\"Unpicked Model Check:\",model_all_core_sep_rev_lid,\"Order:\", model_all_core_sep_rev_lid.order, model_all_core_sep_rev_lid.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_single_text(text):\n",
    "    list_reversed_text = []\n",
    "    # Tokenize to words\n",
    "    # first split the string into chars\n",
    "    chars = text.split(' ')\n",
    "\n",
    "    # then reverse the split string list and join with a space\n",
    "    reversed_text = ' '.join(reversed(chars))\n",
    "    return reversed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Add initial and terminal padding to each text\"\"\"\n",
    "def pad(text):\n",
    "    return \"<s> \" + text + \" </s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b466326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity0(model1, k, this_texts, this_tokenized_text,seed=8):\n",
    "    perplexity_list = []\n",
    "    try:\n",
    "        this_data_list = []\n",
    "        this_data_list, _= padded_everygram_pipeline(k, this_tokenized_text)\n",
    "        for i, test in enumerate(this_data_list):\n",
    "            if(this_texts[i]!=\" \"):\n",
    "                perp = model1.perplexity(test)\n",
    "                #print(\"Test:\", test,\"Perpleixty:\", perp)\n",
    "                if(perp>100000): perp = 100000\n",
    "            else: perp = 0\n",
    "            perplexity_list.append(perp)\n",
    "            #print(\"Perplexity( {0}):{1}\".format(this_texts[i], model1.perplexity(test)))\n",
    "    except Exception as e:\n",
    "                print(\"Exception:\", e.__class__, \"get_perplexity\")\n",
    "                traceback.print_exc()\n",
    "    return this_texts, perplexity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75719b97",
   "metadata": {},
   "source": [
    "# Anamolous texts - Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6677ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running a test for anamolous texts\n",
    "def run_test_anamolous_texts(test_name,data_to_use, model_fwd, model_rev, order, a, a_rev,seed):\n",
    "    \n",
    "    verbose_debug= True\n",
    "    quadgram_models= False\n",
    "\n",
    "\n",
    "    print(\"_____________________________\")\n",
    "    print(\"_____ Running \", test_name, \"_________\")\n",
    "    print(\"_____________________________\")\n",
    "\n",
    "    this_tokenized_text = None\n",
    "    this_text_list =[]\n",
    "    this_tokenized_text_list=[]\n",
    "    text_list = []\n",
    "    perplexity_list_fwd_text = []\n",
    "    this_tokenized_text_list = []\n",
    "    text_len_list=[]\n",
    "        \n",
    "    out_dict =  defaultdict()\n",
    "    net_dict = defaultdict()\n",
    "    metrics_list = None\n",
    "\n",
    "\n",
    "    try_reverse = False\n",
    "\n",
    "    try:\n",
    "\n",
    "        \"\"\" Code to run the Perplexity text and return results\"\"\"\n",
    "\n",
    "        if(use_fwd_model):\n",
    "            this_text_list = list(a)\n",
    "            model_in = model_fwd\n",
    "        else:\n",
    "            this_text_list = list(a_rev)\n",
    "            model_in = model_rev\n",
    "\n",
    "        for text in a:\n",
    "\n",
    "            \"\"\"Remove the padding as get_perplexity0 needs unpadded text\"\"\"\n",
    "            text_without_padding = text.replace('<s>', \"\")\n",
    "            text_without_padding = text_without_padding.replace('</s>', \"\")\n",
    "\n",
    "            this_tokenized_text= word_tokenize(text_without_padding)\n",
    "            text_len_list.append(len(this_tokenized_text))\n",
    "\n",
    "            this_tokenized_text_list.append(this_tokenized_text)\n",
    "            #print(this_tokenized_text_list)\n",
    "\n",
    "\n",
    "        text_list, perplexity_list_fwd_text = get_perplexity0(model_in, order, this_text_list, this_tokenized_text_list,seed=8)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e.__class__, \"run_test_anamolous_texts\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return  text_list, text_len_list, perplexity_list_fwd_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e395402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows,operation, seed):\n",
    "\n",
    "    list_changed_texts = []\n",
    "    list_changed_reversed_text = []\n",
    "    row_count=0\n",
    "    ls_made_up_row = []\n",
    "    random.seed(seed)\n",
    "    df_made_up = None\n",
    "\n",
    "    try:\n",
    "        a = df_in[df_in.l_to_r_text!=''].l_to_r_text\n",
    "        \n",
    "        df_in = df_in.reset_index() #indexes should pair with number of rows\n",
    "\n",
    "        \n",
    "        for index, row in df_in.iterrows():\n",
    "            \"\"\" Tokenize to words, first split the string into chars \"\"\"\n",
    "            text = row['l_to_r_text']\n",
    "            site = row['site']\n",
    "            icit_id = row['icit_id']\n",
    "            \n",
    "            chars = text.split(' ')\n",
    "            new_text = chars\n",
    "            \n",
    "            if(operation==\"shuffle\"):\n",
    "                # Randomly shuffle the list\n",
    "                random.shuffle(new_text)\n",
    "            elif(operation==\"random\"):\n",
    "                # Randomly generate signs\n",
    "                for i in range(0, len(new_text)):\n",
    "                    rand_sign = random.randint(0, 709)\n",
    "                    new_text[i] = f\"{rand_sign:03}\"\n",
    "                \n",
    "            #print(\"text, len(chars),max_text_chars,min_text_chars,row_count\",text, len(chars),max_text_chars,min_text_chars,row_count)\n",
    "\n",
    "            if(len(chars)<=max_text_chars and len(chars)>=min_text_chars) :\n",
    "\n",
    "                changed_text = ' '.join((new_text))\n",
    "\n",
    "                made_up_row= {'site' : site,\n",
    "                   'changed_reversed_text'  : pad(reverse_single_text(changed_text)),\n",
    "                   'changed_text' : pad(changed_text),\n",
    "                   'icit_id' : icit_id}\n",
    "\n",
    "                ls_made_up_row.append(made_up_row)\n",
    "\n",
    "\n",
    "                row_count=row_count+1\n",
    "                if(row_count>=max_num_of_rows): break\n",
    "\n",
    "\n",
    "        df_made_up = pd.DataFrame(ls_made_up_row)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e.__class__, \"prepare_data_anamolous_texts\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return df_made_up\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test_anamolous_texts_run\n",
    "\n",
    "Function to Run the test\n",
    "\"\"\" \n",
    "def test_anamolous_texts_run(df_this,model_fwd, model_rev, seed):\n",
    "    verbose_debug = True\n",
    "    \n",
    "    text_list = []\n",
    "    perplexity_list_fwd_text = []\n",
    "    text_len_list= []\n",
    "    \n",
    "    try:\n",
    "        a = df_this[df_this.changed_text!=''].changed_text\n",
    "        a_rev = df_this[df_this.changed_reversed_text!=''].changed_reversed_text\n",
    "\n",
    "        text_list,text_len_list, perplexity_list_fwd_text = run_test_anamolous_texts(\"test_Anamolous_texts_run\",\"Data\",model_fwd, model_rev, model_order, a, a_rev,seed )\n",
    "    \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e.__class__, \"test_Anamolous_texts_run\")\n",
    "        traceback.print_exc()\n",
    "    return text_list, text_len_list, perplexity_list_fwd_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675e306",
   "metadata": {},
   "source": [
    "## Anamolous Texts Test 1 - With Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set the model and Model order to use\"\"\"\n",
    "\n",
    "use_fwd_model = True # Use Forward model\n",
    "find_best_model= False # Turn this to True when trying to find best model. Used in Test 2\n",
    "model_order=7\n",
    "\n",
    "model_name= \"Lidstone\"\n",
    "index = model_name_list.index(model_name)\n",
    "        \n",
    "model_fwd = model_core_fwd[index][model_order]\n",
    "model_rev = model_core_rev[index][model_order]\n",
    "\n",
    "\"\"\"This is the best model selected based on tests\"\"\"\n",
    "model_all_fwd =  model_all_core_sep_fwd_lid #model_fwd\n",
    "model_all_rev =  model_all_core_sep_rev_lid #model_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13990bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare and Run Anamolous_text_Test1\"\"\"\n",
    "\n",
    "verbose_debug= True\n",
    "min_text_chars = 2\n",
    "max_text_chars = 40\n",
    "max_num_of_rows= 4000\n",
    "\n",
    "text_len_list=[]\n",
    "df_result_train= df = pd.DataFrame()\n",
    "\n",
    "\n",
    "df_in = df_core_train\n",
    "\n",
    "\"\"\"Set the number of runs\"\"\"\n",
    "run_num_of_times_train= 1\n",
    "operation='None'\n",
    "\n",
    "for x in range(1,run_num_of_times_train+1):\n",
    "    df_out= None\n",
    "    this_seed = x\n",
    "    random.seed(x)\n",
    "    df_out_train= prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows, operation, this_seed )\n",
    "    #print(df_out_train)\n",
    "    \n",
    "    text_list_train,text_len_list_train, perplexity_list_fwd_text_train = test_anamolous_texts_run(df_out_train,model_fwd, model_rev, this_seed)\n",
    "   \n",
    "    data_type_list_train = ['train'] * len(text_list_train)\n",
    "    df_result_train['data']=  data_type_list_train\n",
    "    df_result_train['text_len'] = text_len_list_train\n",
    "    df_result_train['perplexity'] = perplexity_list_fwd_text_train\n",
    "    df_result_train['text'] = df_out_train['changed_text'].tolist()\n",
    "    df_result_train['site'] = df_out_train['site'].tolist() \n",
    "    df_result_train['icit_id'] = df_out_train['icit_id'].tolist() \n",
    "    #print(df_result_train.to_string())\n",
    "    df_result_train['rev_text'] = df_out_train['changed_reversed_text'].tolist()\n",
    "\n",
    "    \n",
    "    stats_numeric_train = df_result_train['perplexity'].describe()\n",
    "    print (stats_numeric_train)\n",
    "    print(\"median  \", df_result_train['perplexity'].median())\n",
    "    \n",
    "    df_result_high_perp_train = df_result_train[df_result_train['perplexity'] > 100]\n",
    "    print(\"\\n-----High Peplexity Texts-----\")\n",
    "    print(df_result_high_perp_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Scatter plot the Perplexity\"\"\"\n",
    "\n",
    "plt.figure(1,figsize=(10,4))\n",
    "plt.scatter(df_result_train['text_len'], df_result_train['perplexity'],  s=20, c='green')\n",
    "plt.xlabel('Text length')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50979873",
   "metadata": {},
   "source": [
    "## Anamolous Texts Test 2 - With Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare and Run Anamolous_text_Test2\"\"\"\n",
    "\n",
    "verbose_debug= True\n",
    "\n",
    "min_text_chars = 2\n",
    "max_text_chars = 40\n",
    "max_num_of_rows= 400\n",
    "text_len_list=[]\n",
    "df_result_test= df = pd.DataFrame()\n",
    "\n",
    "\n",
    "df_in = df_core_test\n",
    "\n",
    "\"\"\"Set the number of runs\"\"\"\n",
    "run_num_of_times_test= 1\n",
    "operation='None'\n",
    "\n",
    "for x in range(1,run_num_of_times_test+1):\n",
    "    \n",
    "    df_out_test= None\n",
    "    this_seed = x\n",
    "    random.seed(x)\n",
    "    df_out_test= prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows,operation, this_seed )\n",
    "    print(df_out_test)\n",
    "\n",
    "    \"\"\" For Test 2, we run all model and all order to capture which model has smallest Median and Mean perplexity\n",
    "    That model is then trained on All Core data and tested again Indus Non core region texts later\"\"\"\n",
    "    \"\"\" For every model and order get the data for Test set\"\"\"\n",
    "\n",
    "    if(find_best_model):\n",
    "        for model_name in model_name_list:\n",
    "            index = model_name_list.index(model_name)\n",
    "            for model_order in range(2, 9):\n",
    "                model_test2_fwd = model_core_fwd[index][model_order]\n",
    "                model_test2_rev = model_core_rev[index][model_order]\n",
    "                print(\"---\", model_name, model_order , \"---\")\n",
    "\n",
    "                text_list_test,text_len_list_test, perplexity_list_fwd_text_test = test_anamolous_texts_run(df_out_test,model_test2_fwd, model_test2_rev, this_seed)\n",
    "                data_type_list_test = ['test'] * len(text_list_test)\n",
    "                df_result_test['data']=  data_type_list_test\n",
    "                df_result_test['text_len'] = text_len_list_test\n",
    "                df_result_test['perplexity'] = perplexity_list_fwd_text_test\n",
    "                df_result_test['text'] = df_out_test['changed_text'].tolist()\n",
    "                df_result_test['site'] = df_out_test['site'].tolist() \n",
    "                df_result_test['icit_id'] = df_out_test['icit_id'].tolist() \n",
    "\n",
    "                #print(df_result_test.to_string())\n",
    "                df_result_test['rev_text'] = df_out_test['changed_reversed_text'].tolist() \n",
    "\n",
    "\n",
    "                stats_numeric_test = df_result_test['perplexity'].describe()\n",
    "                print (stats_numeric_test)\n",
    "                print(\"median  \", df_result_test['perplexity'].median())\n",
    "\n",
    "                df_result_high_perp_test = df_result_test[df_result_test['perplexity'] > 100]\n",
    "                #print(\"\\n-----High Peplexity Texts-----\")\n",
    "                #print(df_result_high_perp_test.to_string())\n",
    "    else:\n",
    "        \n",
    "        \"\"\" If not running test to find best model\"\"\"\n",
    "        text_list_test,text_len_list_test, perplexity_list_fwd_text_test = test_anamolous_texts_run(df_out_test,model_fwd, model_rev, this_seed)\n",
    "\n",
    "        data_type_list_test = ['test'] * len(text_list_test)\n",
    "        df_result_test['data']=  data_type_list_test\n",
    "        df_result_test['text_len'] = text_len_list_test\n",
    "        df_result_test['perplexity'] = perplexity_list_fwd_text_test\n",
    "        df_result_test['text'] = df_out_test['changed_text'].tolist()\n",
    "        df_result_test['site'] = df_out_test['site'].tolist() \n",
    "        df_result_test['icit_id'] = df_out_test['icit_id'].tolist() \n",
    "\n",
    "        #print(df_result_test.to_string())\n",
    "        df_result_test['rev_text'] = df_out_test['changed_reversed_text'].tolist() \n",
    "\n",
    "\n",
    "        stats_numeric_test = df_result_test['perplexity'].describe()\n",
    "        print (stats_numeric_test)\n",
    "        print(\"median  \", df_result_test['perplexity'].median())\n",
    "        print(\"90 the Percentile \", df_result_test['perplexity'].quantile(0.9))\n",
    "\n",
    "        df_result_high_perp_test = df_result_test[df_result_test['perplexity'] > 100]\n",
    "        #print(\"\\n-----High Peplexity Texts-----\")\n",
    "        #print(df_result_high_perp_test.to_string())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Scatter plot the Perplexity\"\"\"\n",
    "\n",
    "plt.figure(2,figsize=(10,4))\n",
    "plt.scatter(df_result_test['text_len'], df_result_test['perplexity'],  s=20, c='red')\n",
    "plt.xlabel('Text length')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a2562",
   "metadata": {},
   "source": [
    "## Anamolous Texts Test 3.1 - With Randomly generated Texts - Random Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare and Run Anamolous_text_Test3.1\"\"\"\n",
    "\n",
    "verbose_debug= True\n",
    "\n",
    "min_text_chars = 4\n",
    "max_text_chars = 40\n",
    "max_num_of_rows= 400\n",
    "text_len_list=[]\n",
    "df_result_random1= df = pd.DataFrame()\n",
    "\n",
    "df_in = df_core_test\n",
    "\n",
    "\"\"\"Set the number of runs\"\"\"\n",
    "run_num_of_times_random1= 1\n",
    "operation = \"shuffle\"\n",
    "\n",
    "\n",
    "for x in range(1,run_num_of_times_random1+1):\n",
    "    df_out_random1= None\n",
    "    this_seed = x\n",
    "    random.seed(x)\n",
    "    df_out_random1= prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows, operation, this_seed )\n",
    "    print(df_out_random1)\n",
    "    \n",
    "    text_list_random1,text_len_list_random1, perplexity_list_fwd_text_random1 = test_anamolous_texts_run(df_out_random1,model_fwd, model_rev, this_seed)\n",
    "  \n",
    "   \n",
    "    data_type_list_random1 = ['random1'] * len(text_list_random1)\n",
    "    df_result_random1['data']=  data_type_list_random1\n",
    "    df_result_random1['text_len'] = text_len_list_random1\n",
    "    df_result_random1['perplexity'] = perplexity_list_fwd_text_random1\n",
    "    df_result_random1['text'] = df_out_random1['changed_text'].tolist()\n",
    "    #df_result_random1['site'] = df_out_random1['site'].tolist() \n",
    "    #print(df_result_random1.to_string())\n",
    "    df_result_random1['rev_text'] = df_out_random1['changed_reversed_text'].tolist()\n",
    "    \n",
    "    stats_numeric_random1 = df_result_random1['perplexity'].describe()\n",
    "    print ( stats_numeric_random1)\n",
    "    print(\"median  \", df_result_random1['perplexity'].median())\n",
    "    print(\"90 the Percentile \", df_result_random1['perplexity'].quantile(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Scatter plot the Perplexity\"\"\"\n",
    "\n",
    "plt.figure(3,figsize=(10,4))\n",
    "plt.scatter(df_result_random1['text_len'], df_result_random1['perplexity'],  s=20, c='red')\n",
    "plt.xlabel('Text length')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcddce83",
   "metadata": {},
   "source": [
    "## Anamolous Texts Test 3.2 - With Randomly generated Texts - Full Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare and Run Anamolous_text_Test3.2\"\"\"\n",
    "\n",
    "verbose_debug= True\n",
    "\n",
    "min_text_chars = 4\n",
    "max_text_chars = 40\n",
    "max_num_of_rows= 400\n",
    "text_len_list=[]\n",
    "df_result_random2= df = pd.DataFrame()\n",
    "\n",
    "df_in = df_core_test\n",
    "\n",
    "\"\"\"Set the number of runs\"\"\"\n",
    "run_num_of_times_random2= 1\n",
    "operation = \"random\"\n",
    "\n",
    "\n",
    "for x in range(1,run_num_of_times_random2+1):\n",
    "    df_out_random2= None\n",
    "    this_seed = x\n",
    "    random.seed(x)\n",
    "    df_out_random2= prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows, operation, this_seed )\n",
    "    print(df_out_random2)\n",
    "    \n",
    "    text_list_random2,text_len_list_random2, perplexity_list_fwd_text_random2 = test_anamolous_texts_run(df_out_random2,model_fwd, model_rev, this_seed)\n",
    "  \n",
    "   \n",
    "    data_type_list_random2 = ['random2'] * len(text_list_random2)\n",
    "    df_result_random2['data']=  data_type_list_random2\n",
    "    df_result_random2['text_len'] = text_len_list_random2\n",
    "    df_result_random2['perplexity'] = perplexity_list_fwd_text_random2\n",
    "    df_result_random2['text'] = df_out_random2['changed_text'].tolist()\n",
    "    #df_result_random2['site'] = df_out_random2['site'].tolist() \n",
    "    #print(df_result_random2.to_string())\n",
    "    df_result_random2['rev_text'] = df_out_random2['changed_reversed_text'].tolist()\n",
    "    \n",
    "    stats_numeric_random2 = df_result_random2['perplexity'].describe()\n",
    "    print ( stats_numeric_random2)\n",
    "    print(\"median  \", df_result_random2['perplexity'].median())\n",
    "    print(\"90 the Percentile \", df_result_random2['perplexity'].quantile(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Scatter plot the Perplexity\"\"\"\n",
    "\n",
    "plt.figure(9,figsize=(10,4))\n",
    "plt.scatter(df_result_random2['text_len'], df_result_random2['perplexity'],  s=20, c='red')\n",
    "plt.xlabel('Text length')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb2c53",
   "metadata": {},
   "source": [
    "## Anomalous Texts Test 4 - Indus Non Core Region (West Asia) texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage,AnnotationBbox\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def get_img_urls(sign_number_list):\n",
    "\n",
    "    img_urls = \"<figure class=\\\"item\\\" style=\\\"display:flex\\\">\"\n",
    "\n",
    "    for sign_number in sign_number_list:\n",
    "        if(sign_number != \"<s>\" and sign_number != \"</s>\" ):\n",
    "            img_urls = img_urls + \" \" + \"<img src=\\\"../ICIT/ICIT Sign List/S\" + str(sign_number) + \".jpg\\\" alt=\\\"img\\\" width=\\\"15\\\" height=\\\"10\\\" class=\\\"w3-image\\\">\"\n",
    "            img_urls = img_urls + \" \" + \"<figcaption class=\\\"caption\\\">\" + str(sign_number) + \"</figcaption>\"\n",
    "    img_urls = img_urls + \" \" + \"</figure>\"\n",
    "    return img_urls\n",
    "\n",
    "def display_table(df_inp,caption):\n",
    "    pd.set_option('display.colheader_justify', 'left')\n",
    "    \n",
    "    orig_text_sign = 'text_img'\n",
    "    \n",
    "    df_inp_top_x = df_inp.copy()\n",
    "    df_inp_top_x.style.set_properties(**{'text-align': 'left'})\n",
    "    df_inp_top_x[orig_text_sign] = None\n",
    "\n",
    "    \n",
    "    for i in range(0, len(df_inp_top_x)):\n",
    "        df_inp_top_x.loc[i:,orig_text_sign] = get_img_urls(df_inp_top_x.loc[i]['text'].split())\n",
    "\n",
    "\n",
    "    df_inp_top_x.style.set_caption(caption)\n",
    "    df_inp_top_x.style.set_properties(subset=[orig_text_sign], **{'width-min': '200px'})\n",
    "\n",
    "\n",
    "    display(HTML(df_inp_top_x[[\"icit_id\",\"site\", \"text_len\", \"perplexity\", orig_text_sign]].to_html(escape=False, index=False)))\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebcece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare and Run Anamolous_text_Test4\"\"\"\n",
    "\n",
    "verbose_debug= True\n",
    "\n",
    "min_text_chars = 2\n",
    "max_text_chars = 40\n",
    "max_num_of_rows= 10\n",
    "text_len_list=[]\n",
    "df_result_non_core= df = pd.DataFrame()\n",
    "\n",
    "df_in = df_indus_non_core_region\n",
    "\n",
    "\"\"\"Set the number of runs\"\"\"\n",
    "run_num_of_times_non_core= 1\n",
    "operation = \"None\"\n",
    "\n",
    "\n",
    "for x in range(1,run_num_of_times_non_core+1):\n",
    "    df_out_non_core= None\n",
    "    this_seed = x\n",
    "    random.seed(x)\n",
    "    df_out_non_core= prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows, operation, this_seed )\n",
    "    print(df_out_non_core)\n",
    "    \n",
    "    \"\"\" Notice that the model used here is the best model and is trained on All core data (core train + core test)\"\"\"\n",
    "    text_list_non_core,text_len_list_non_core, perplexity_list_fwd_text_non_core = test_anamolous_texts_run(df_out_non_core,model_all_fwd, model_all_rev, this_seed)\n",
    "  \n",
    "   \n",
    "    data_type_list_non_core = ['non_core'] * len(text_list_non_core)\n",
    "    df_result_non_core['data']=  data_type_list_non_core\n",
    "    df_result_non_core['text_len'] = text_len_list_non_core\n",
    "    df_result_non_core['perplexity'] = perplexity_list_fwd_text_non_core\n",
    "    df_result_non_core['text'] = df_out_non_core['changed_text'].tolist()\n",
    "    df_result_non_core['site'] = df_out_non_core['site'].tolist() \n",
    "    df_result_non_core['icit_id'] =df_out_non_core['icit_id'].tolist() \n",
    "    #print(df_result_non_core.to_html())\n",
    "    from IPython.display import display\n",
    "\n",
    "    #display(df_result_non_core)\n",
    "    display_table(df_result_non_core,\"West Asian Indus Texts\")\n",
    "    \n",
    "    df_result_non_core['rev_text'] = df_out_non_core['changed_reversed_text'].tolist()\n",
    "    \n",
    "    stats_numeric_non_core = df_result_non_core['perplexity'].describe()\n",
    "    print (stats_numeric_non_core)\n",
    "\n",
    "    df_result_high_perp_non_core = df_result_non_core[(df_result_non_core['perplexity'] > 40) & (df_result_non_core['text_len'] > 2) ]\n",
    "    print(\"\\n-----High Peplexity Texts-----\")\n",
    "    print(df_result_high_perp_non_core)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ac645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Scatter plot the Perplexity\"\"\"\n",
    "\n",
    "plt.figure(5,figsize=(10,4))\n",
    "plt.scatter(df_result_non_core['text_len'], df_result_non_core['perplexity'],  s=20, c='red')\n",
    "plt.xlabel('Text length')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc763d19",
   "metadata": {},
   "source": [
    "## Anomalous Texts Test 5 - South India texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" South Indian texts\"\"\"\n",
    "# At this point this has only one South Indian texts\n",
    "df_indus_si_region= df = pd.DataFrame()\n",
    "df_indus_si_region['l_to_r_text'] =[\"<s> 176 740 776 921 </s>\"] # [\"<s> 921 776 740 176\"]\n",
    "df_indus_si_region['site'] = [\"Sembiyan Kandiyur\"]\n",
    "df_indus_si_region['icit_id'] = [\"Undocumented\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare and Run Anamolous_text_Test5\"\"\"\n",
    "\n",
    "verbose_debug= True\n",
    "\n",
    "min_text_chars = 2\n",
    "max_text_chars = 40\n",
    "max_num_of_rows= 10\n",
    "text_len_list=[]\n",
    "df_result_si= df = pd.DataFrame()\n",
    "\n",
    "df_in = df_indus_si_region\n",
    "\n",
    "\"\"\"Set the number of runs\"\"\"\n",
    "run_num_of_times_si= 1\n",
    "operation = \"None\"\n",
    "\n",
    "\n",
    "for x in range(1,run_num_of_times_si+1):\n",
    "    df_out_si= None\n",
    "    this_seed = x\n",
    "    random.seed(x)\n",
    "    df_out_si= prepare_data_anamolous_texts(df_in,max_text_chars,min_text_chars,max_num_of_rows, operation, this_seed )\n",
    "    print(df_out_si)\n",
    "    \n",
    "    \"\"\" Notice that the model used here is the best model and is trained on All core data (core train + core test)\"\"\"\n",
    "    text_list_si,text_len_list_si, perplexity_list_fwd_text_si = test_anamolous_texts_run(df_out_si,model_all_fwd, model_all_rev, this_seed)\n",
    "  \n",
    "   \n",
    "    data_type_list_si = ['si'] * len(text_list_si)\n",
    "    df_result_si['data']=  data_type_list_si\n",
    "    df_result_si['text_len'] = text_len_list_si\n",
    "    df_result_si['perplexity'] = perplexity_list_fwd_text_si\n",
    "    df_result_si['text'] = df_out_si['changed_text'].tolist()\n",
    "    df_result_si['site'] = df_out_si['site'].tolist() \n",
    "    df_result_si['icit_id'] =df_out_si['icit_id'].tolist() \n",
    "    #print(df_result_si.to_html())\n",
    "    from IPython.display import display\n",
    "\n",
    "    #display(df_result_si)\n",
    "    display_table(df_result_si,\"South India Indus Texts\")\n",
    "    \n",
    "    df_result_si['rev_text'] = df_out_si['changed_reversed_text'].tolist()\n",
    "    \n",
    "    stats_numeric_si = df_result_si['perplexity'].describe()\n",
    "    print (stats_numeric_si)\n",
    "\n",
    "    #For Lidstone n=7\n",
    "    df_result_high_perp_si = df_result_si[(df_result_si['perplexity'] > 40) & (df_result_si['text_len'] > 2) ]\n",
    "    print(\"\\n-----High Peplexity Texts-----\")\n",
    "    print(df_result_high_perp_si)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3f2b0",
   "metadata": {},
   "source": [
    "## All anomalous texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cdb6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Concatanate the High Perplexity text rows from train, test and actual non_core\"\"\"\n",
    "\n",
    "frames = [df_result_high_perp_train, df_result_high_perp_test, df_result_high_perp_non_core]\n",
    "\n",
    "df_result_high_perp_all= pd.concat(frames)\n",
    "print(df_result_high_perp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_result_train['perplexity'])\n",
    "plt.figtext(0.70,0.5, df_result_train['perplexity'].describe().to_string())\n",
    "\n",
    "plt.xlabel(\"Perplexity\",  fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.title(\"Perplexity for Training set\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_result_test['perplexity'])\n",
    "plt.figtext(0.70,0.5, df_result_test['perplexity'].describe().to_string())\n",
    "\n",
    "plt.xlabel(\"Perplexity\",  fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.title(\"Perplexity for Test set\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32359bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_result_random1['perplexity'])\n",
    "plt.figtext(0.70,0.5, df_result_random1['perplexity'].describe().to_string())\n",
    "\n",
    "plt.xlabel(\"Perplexity\",  fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.title(\"Perplexity for Random data set\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_result_non_core['perplexity'])\n",
    "plt.figtext(0.70,0.5, df_result_non_core['perplexity'].describe().to_string())\n",
    "\n",
    "plt.xlabel(\"Perplexity\",  fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.title(\"Perplexity for Indus Non Core region data set\", fontsize=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
