{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d9262f",
   "metadata": {},
   "source": [
    "# Indus Valley Script- Text Analysis for Decipherment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5247cd2",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "This file is used to build and train the language models from cleaned up data i.e dataframes and pickle them\n",
    "\n",
    "Dataset was created as a csv file from ICIT web site from raw html files of ICIT code for each for the Text\n",
    "Data labels were changes and a linearized copy of the original text was added\n",
    "\n",
    "### Input:\n",
    "Pickled data file from Input Data Processing\n",
    "\n",
    "### Output:\n",
    "Pickled model files from Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5b49",
   "metadata": {},
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install ipywidgets\n",
    "!pip install -U dill\n",
    "!pip3 install requests\n",
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa43bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import collections\n",
    "import random\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "#plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbae861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from collections import defaultdict\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e754e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8\n",
    "seed = 8\n",
    "CONST_INITIAL = \"Initial\"\n",
    "CONST_TERMINAL = \"Terminal\"\n",
    "CONST_MEDIAL = \"Medial\"\n",
    "CONST_NL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e0c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sign df: \n",
      "     id_sign sign_class set   graph       type        image variants  \\\n",
      "0         1        SIM  01  stroke     stroke  sign001.jpg        1   \n",
      "1         2        MKR  01  stroke     stroke  sign002.jpg        1   \n",
      "2         3        SIM  01  stroke     stroke  sign003.jpg        1   \n",
      "3         4        SIM  01  stroke     stroke  sign004.jpg        1   \n",
      "4         5        SIM  01  stroke     stroke  sign005.jpg        1   \n",
      "..      ...        ...  ..     ...        ...          ...      ...   \n",
      "704     952        CMX  71  animal  uncertain  sign952.jpg        1   \n",
      "705     953        CMX  71  animal       Pict  sign953.jpg        1   \n",
      "706     956        SIM  71       -    att.d.e  sign956.jpg        1   \n",
      "707     957        CMX  71       -  uncertain  sign957.jpg        1   \n",
      "708     958        CMX  71       -  uncertain  sign958.jpg        1   \n",
      "\n",
      "          function ligatur value frequency comment  \n",
      "0    NUM, ITM, SHN       -     -       227       -  \n",
      "1    ITM, SHN, EMS       -     -       865       -  \n",
      "2         NUM, SHN       -     -       260       -  \n",
      "3         NUM, SHN       -     -        99       -  \n",
      "4         NUM, SHN       -     -        49       -  \n",
      "..             ...     ...   ...       ...     ...  \n",
      "704            LFS       -     -         1       -  \n",
      "705            LFS       -     -         1       -  \n",
      "706            LOG       -     -         2       -  \n",
      "707            LOG       -     -         2       -  \n",
      "708            LFS       -     -         1       -  \n",
      "\n",
      "[709 rows x 12 columns]\n",
      "Updated original text df: \n",
      "      icit_id        site keywords text_class lines direction       text signs  \\\n",
      "0          1  Alamgirpur      NaN         SS     1       L/R  +410-017+     2   \n",
      "1          2  Alamgirpur      NaN         SS     1       L/R  +410-017+     2   \n",
      "2          3  Alamgirpur      NaN         SC     1       L/R  +405-017+     2   \n",
      "3          4   Allahdino      NaN         ??     1       NaN  +220-000+     1   \n",
      "4          5   Allahdino     Bull         UC     1       R/L  +740-235+     2   \n",
      "...      ...         ...      ...        ...   ...       ...        ...   ...   \n",
      "4994    4064     Harappa      NaN         UC     1       NaN      +000[     0   \n",
      "4995    4065     Harappa      NaN         VN     1       R/L  ]700-032[     2   \n",
      "4996    4065     Harappa      NaN         UC     1       R/L  ]000-000[     0   \n",
      "4997    4066     Harappa      NaN         UC     1       R/L  +368-000+     1   \n",
      "4998    4066     Harappa      NaN         VN     1       R/L  +700-033+     2   \n",
      "\n",
      "     complete    alignment  sign height text_images linearized_text  \\\n",
      "0           Y    Unordered      Unequal         NaN         410 017   \n",
      "1           Y          NaN          NaN         NaN         410 017   \n",
      "2           Y          NaN          NaN         NaN         405 017   \n",
      "3           N          NaN          NaN         NaN         220 000   \n",
      "4           Y          NaN          NaN         NaN         740 235   \n",
      "...       ...          ...          ...         ...             ...   \n",
      "4994        N  Indefinable  Indefinable         NaN            000[   \n",
      "4995        ?          NaN          NaN         NaN        700 032[   \n",
      "4996        N          NaN          NaN         NaN        000 000[   \n",
      "4997        N          NaN          NaN         NaN         368 000   \n",
      "4998        Y          NaN          NaN         NaN         700 033   \n",
      "\n",
      "     l_to_r_text r_to_l_text reversed_text  text_length  \n",
      "0        410 017     017 410       017 410          2.0  \n",
      "1        410 017     017 410       017 410          2.0  \n",
      "2        405 017     017 405       017 405          2.0  \n",
      "3        000 220     000 220       220 000          2.0  \n",
      "4        235 740     740 235       740 235          2.0  \n",
      "...          ...         ...           ...          ...  \n",
      "4994        000[        000[          000[          1.0  \n",
      "4995    032[ 700    700 032[      700 032[          3.0  \n",
      "4996    000[ 000    000 000[      000 000[          3.0  \n",
      "4997     000 368     368 000       368 000          2.0  \n",
      "4998     033 700     700 033       700 033          2.0  \n",
      "\n",
      "[4999 rows x 17 columns]\n",
      "Cleaned text df: \n",
      "      icit_id          site keywords text_class lines direction  \\\n",
      "0          1    Alamgirpur      NaN         SS     1       L/R   \n",
      "2          3    Alamgirpur      NaN         SC     1       L/R   \n",
      "4          5     Allahdino     Bull         UC     1       R/L   \n",
      "5          6     Allahdino    Bull1         SS     1       R/L   \n",
      "6          7     Allahdino    Bull1         SC     1       R/L   \n",
      "...      ...           ...      ...        ...   ...       ...   \n",
      "4953    4033  Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4956    4036  Mohenjo-daro      NaN         SS     1       R/L   \n",
      "4959    4039  Mohenjo-daro      NaN         SC     1       R/L   \n",
      "4967    4047  Mohenjo-daro      NaN         SC     1       R/L   \n",
      "4985    4059       Harappa      NaN         SP     1       R/L   \n",
      "\n",
      "                       text signs complete      alignment sign height  \\\n",
      "0                 +410-017+     2        Y      Unordered     Unequal   \n",
      "2                 +405-017+     2        Y            NaN         NaN   \n",
      "4                 +740-235+     2        Y            NaN         NaN   \n",
      "5             +740-390-590+     3        Y  Strong linear       Equal   \n",
      "6         +368-390-125-033+     4        Y         Linear       Equal   \n",
      "...                     ...   ...      ...            ...         ...   \n",
      "4953          +740-482-838+     3        Y            NaN         NaN   \n",
      "4956          +400-740-176+     3        Y            NaN         NaN   \n",
      "4959          +723-066-828+     3        Y         Linear       Equal   \n",
      "4967      +041-705-002-905+     4        Y            NaN         NaN   \n",
      "4985  +740-061-001-031-820+     5        Y            NaN         NaN   \n",
      "\n",
      "     text_images      linearized_text          l_to_r_text  \\\n",
      "0            NaN              410 017              410 017   \n",
      "2            NaN              405 017              405 017   \n",
      "4            NaN              740 235              235 740   \n",
      "5            NaN          740 390 590          590 390 740   \n",
      "6            NaN      368 390 125 033      033 125 390 368   \n",
      "...          ...                  ...                  ...   \n",
      "4953         NaN          740 482 838          838 482 740   \n",
      "4956         NaN          400 740 176          176 740 400   \n",
      "4959         NaN          723 066 828          828 066 723   \n",
      "4967         NaN      041 705 002 905      905 002 705 041   \n",
      "4985         NaN  740 061 001 031 820  820 031 001 061 740   \n",
      "\n",
      "              r_to_l_text        reversed_text  text_length  \n",
      "0                 017 410              017 410          2.0  \n",
      "2                 017 405              017 405          2.0  \n",
      "4                 740 235              740 235          2.0  \n",
      "5             740 390 590          740 390 590          4.0  \n",
      "6         368 390 125 033      368 390 125 033          5.0  \n",
      "...                   ...                  ...          ...  \n",
      "4953          740 482 838          740 482 838          4.0  \n",
      "4956          400 740 176          400 740 176          4.0  \n",
      "4959          723 066 828          723 066 828          4.0  \n",
      "4967      041 705 002 905      041 705 002 905          5.0  \n",
      "4985  740 061 001 031 820  740 061 001 031 820          6.0  \n",
      "\n",
      "[2223 rows x 17 columns]\n",
      "Unclear text df: \n",
      "      icit_id           site keywords text_class lines direction  \\\n",
      "12        13      Allahdino     Elep         UC     1       R/L   \n",
      "20        21       Bala-kot    Bull1         UC     1       R/L   \n",
      "22        23       Bala-kot      NaN         UC     1       R/L   \n",
      "61        62  Chanhujo-daro  Bull1:W         UC     1       R/L   \n",
      "81        82  Chanhujo-daro  Bull1:W         LP     1       R/L   \n",
      "...      ...            ...      ...        ...   ...       ...   \n",
      "4955    4035   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4958    4038   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4961    4041   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4965    4045   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4997    4066        Harappa      NaN         UC     1       R/L   \n",
      "\n",
      "                               text signs complete alignment sign height  \\\n",
      "12        +740-440-503-002-861-000+     5        N       NaN         NaN   \n",
      "20            +000-388-233-000-000+     2        N    Linear       Equal   \n",
      "22                    +750-790-000+     2        N       NaN         NaN   \n",
      "61    +000-100-240-220-032-002-861+     6        N    Linear       Equal   \n",
      "81    +740-032-840-002-296-530-000+     6        N    Linear     Adapted   \n",
      "...                             ...   ...      ...       ...         ...   \n",
      "4955                  +000-035-032+     2        N       NaN         NaN   \n",
      "4958                      +018-000+     1        N       NaN         NaN   \n",
      "4961          +400-740-760-000-000+     3        N       NaN         NaN   \n",
      "4965  +000-000-095-550-003-002-000+     4        N       NaN         NaN   \n",
      "4997                      +368-000+     1        N       NaN         NaN   \n",
      "\n",
      "     text_images              linearized_text                  l_to_r_text  \\\n",
      "12           NaN      740 440 503 002 861 000      000 861 002 503 440 740   \n",
      "20           NaN          000 388 233 000 000          000 000 233 388 000   \n",
      "22           NaN                  750 790 000                  000 790 750   \n",
      "61           NaN  000 100 240 220 032 002 861  861 002 032 220 240 100 000   \n",
      "81           NaN  740 032 840 002 296 530 000  000 530 296 002 840 032 740   \n",
      "...          ...                          ...                          ...   \n",
      "4955         NaN                  000 035 032                  032 035 000   \n",
      "4958         NaN                      018 000                      000 018   \n",
      "4961         NaN          400 740 760 000 000          000 000 760 740 400   \n",
      "4965         NaN  000 000 095 550 003 002 000  000 002 003 550 095 000 000   \n",
      "4997         NaN                      368 000                      000 368   \n",
      "\n",
      "                      r_to_l_text                reversed_text  text_length  \n",
      "12        740 440 503 002 861 000      740 440 503 002 861 000          8.0  \n",
      "20            000 388 233 000 000          000 388 233 000 000          6.0  \n",
      "22                    750 790 000                  750 790 000          4.0  \n",
      "61    000 100 240 220 032 002 861  000 100 240 220 032 002 861          9.0  \n",
      "81    740 032 840 002 296 530 000  740 032 840 002 296 530 000          9.0  \n",
      "...                           ...                          ...          ...  \n",
      "4955                  000 035 032                  000 035 032          4.0  \n",
      "4958                      018 000                      018 000          2.0  \n",
      "4961          400 740 760 000 000          400 740 760 000 000          6.0  \n",
      "4965  000 000 095 550 003 002 000  000 000 095 550 003 002 000          9.0  \n",
      "4997                      368 000                      368 000          2.0  \n",
      "\n",
      "[291 rows x 17 columns]\n",
      "Multi-line text df: \n",
      "      icit_id           site                          keywords text_class  \\\n",
      "69        70  Chanhujo-daro                          Bull1:II         2L   \n",
      "71        72  Chanhujo-daro                              Gaur         2L   \n",
      "72        73  Chanhujo-daro                             Bull1         2L   \n",
      "74        75  Chanhujo-daro                               NaN         2L   \n",
      "80        81  Chanhujo-daro                               NaN         2L   \n",
      "...      ...            ...                               ...        ...   \n",
      "4386    3538   Mohenjo-daro       Scene, Anth, Phyt, CompBull         2L   \n",
      "4402    3552   Mohenjo-daro                              Gaur         2L   \n",
      "4705    3814   Mohenjo-daro                               NaN         2L   \n",
      "4729    3836   Mohenjo-daro                               NaN         2L   \n",
      "4752    3857       Nausharo  Horned Composition of Anth, Tigr         2L   \n",
      "\n",
      "     lines direction                                       text signs  \\\n",
      "69       2       R/L                  +032-031/151-740-240-235+     6   \n",
      "71       2       R/L          +032-031/850-032-530-740-741-456+     8   \n",
      "72       2       R/L                      +032-031/740-791-713+     5   \n",
      "74       2       L/R                          +032/226-032-817+     4   \n",
      "80       2       R/L                      +740-636-240/002-817+     5   \n",
      "...    ...       ...                                        ...   ...   \n",
      "4386     2       NaN                  +621/090-740-231-560-534+     6   \n",
      "4402     2       R/L              +790/740-100-415-740-257-840+     7   \n",
      "4705     2       R/L                      +740-900-003/741-002+     5   \n",
      "4729     2       NaN                                  +840/790+     2   \n",
      "4752     2       BUS  +605-740-142-067/002-374-310-350-495-834+    10   \n",
      "\n",
      "     complete      alignment sign height text_images  \\\n",
      "69          Y         Linear     Unequal         NaN   \n",
      "71          Y      Unordered     Unequal         NaN   \n",
      "72          Y      Unordered     Unequal         NaN   \n",
      "74          Y         Linear       Equal         NaN   \n",
      "80          Y         Linear       Equal         NaN   \n",
      "...       ...            ...         ...         ...   \n",
      "4386        Y      Unordered     Unequal         NaN   \n",
      "4402        Y         Linear     Unequal         NaN   \n",
      "4705        Y            NaN         NaN         NaN   \n",
      "4729        Y            NaN         NaN         NaN   \n",
      "4752        Y  Partly linear     Unequal         NaN   \n",
      "\n",
      "                              linearized_text  \\\n",
      "69                    032 031 151 740 240 235   \n",
      "71            032 031 850 032 530 740 741 456   \n",
      "72                        032 031 740 791 713   \n",
      "74                            032 226 032 817   \n",
      "80                        740 636 240 002 817   \n",
      "...                                       ...   \n",
      "4386                  621 090 740 231 560 534   \n",
      "4402              790 740 100 415 740 257 840   \n",
      "4705                      740 900 003 741 002   \n",
      "4729                                  840 790   \n",
      "4752  605 740 142 067 002 374 310 350 495 834   \n",
      "\n",
      "                                  l_to_r_text  \\\n",
      "69                    235 240 740 151 031 032   \n",
      "71            456 741 740 530 032 850 031 032   \n",
      "72                        713 791 740 031 032   \n",
      "74                            032 226 032 817   \n",
      "80                        817 002 240 636 740   \n",
      "...                                       ...   \n",
      "4386                  534 560 231 740 090 621   \n",
      "4402              840 257 740 415 100 740 790   \n",
      "4705                      002 741 003 900 740   \n",
      "4729                                  790 840   \n",
      "4752  834 495 350 310 374 002 067 142 740 605   \n",
      "\n",
      "                                  r_to_l_text  \\\n",
      "69                    032 031 151 740 240 235   \n",
      "71            032 031 850 032 530 740 741 456   \n",
      "72                        032 031 740 791 713   \n",
      "74                            817 032 226 032   \n",
      "80                        740 636 240 002 817   \n",
      "...                                       ...   \n",
      "4386                  534 560 231 740 090 621   \n",
      "4402              790 740 100 415 740 257 840   \n",
      "4705                      740 900 003 741 002   \n",
      "4729                                  790 840   \n",
      "4752  834 495 350 310 374 002 067 142 740 605   \n",
      "\n",
      "                                reversed_text  text_length  \n",
      "69                    032 031 151 740 240 235          8.0  \n",
      "71            032 031 850 032 530 740 741 456         10.0  \n",
      "72                        032 031 740 791 713          6.0  \n",
      "74                            817 032 226 032          5.0  \n",
      "80                        740 636 240 002 817          6.0  \n",
      "...                                       ...          ...  \n",
      "4386                  621 090 740 231 560 534          8.0  \n",
      "4402              790 740 100 415 740 257 840          9.0  \n",
      "4705                      740 900 003 741 002          6.0  \n",
      "4729                                  840 790          2.0  \n",
      "4752  605 740 142 067 002 374 310 350 495 834         13.0  \n",
      "\n",
      "[77 rows x 17 columns]\n",
      "all df: \n",
      "               l_to_r_text direction icit_id          site\n",
      "0                 410 017       L/R       1    Alamgirpur\n",
      "1                 405 017       L/R       3    Alamgirpur\n",
      "2                 235 740       R/L       5     Allahdino\n",
      "3             590 390 740       R/L       6     Allahdino\n",
      "4         033 125 390 368       R/L       7     Allahdino\n",
      "...                   ...       ...     ...           ...\n",
      "2218          838 482 740       R/L    4033  Mohenjo-daro\n",
      "2219          176 740 400       R/L    4036  Mohenjo-daro\n",
      "2220          828 066 723       R/L    4039  Mohenjo-daro\n",
      "2221      905 002 705 041       R/L    4047  Mohenjo-daro\n",
      "2222  820 031 001 061 740       R/L    4059       Harappa\n",
      "\n",
      "[2223 rows x 4 columns]\n",
      "train df: \n",
      "                       l_to_r_text direction icit_id          site\n",
      "0                             013        NR    2032        Lothal\n",
      "1                         700 034       R/L     938       Harappa\n",
      "2                     590 407 740       R/L    1874         Hulas\n",
      "3         820 002 806 590 405 740       R/L    3611  Mohenjo-daro\n",
      "4     140 920 484 337 503 456 400       R/L    3578  Mohenjo-daro\n",
      "...                           ...       ...     ...           ...\n",
      "1773              240 100 740 400       R/L    1276       Harappa\n",
      "1774              840 032 740 400       R/L     567       Harappa\n",
      "1775              370 002 550 527       R/L    3670  Mohenjo-daro\n",
      "1776              235 220 222 740       R/L    3773  Mohenjo-daro\n",
      "1777      820 002 803 032 384 740       R/L    3295  Mohenjo-daro\n",
      "\n",
      "[1778 rows x 4 columns]\n",
      "test df: \n",
      "                      l_to_r_text direction icit_id           site\n",
      "0                        003 390       R/L    2191   Mohenjo-daro\n",
      "1                    235 240 520       R/L    1189        Harappa\n",
      "2    861 002 003 220 590 405 740       R/L    1175        Harappa\n",
      "3                            820        NR     912        Harappa\n",
      "4    140 287 002 415 220 879 740       R/L    2636   Mohenjo-daro\n",
      "..                           ...       ...     ...            ...\n",
      "440                      003 390       T/B    1924     Kalibangan\n",
      "441              632 390 400 375       R/L     114  Chanhujo-daro\n",
      "442      861 002 705 255 740 090       R/L    2627   Mohenjo-daro\n",
      "443                  176 740 400       R/L     260        Harappa\n",
      "444          317 920 002 016 740       R/L      91  Chanhujo-daro\n",
      "\n",
      "[445 rows x 4 columns]\n",
      "all_rev df: \n",
      "             reversed_text          site\n",
      "0                 017 410    Alamgirpur\n",
      "1                 017 405    Alamgirpur\n",
      "2                 740 235     Allahdino\n",
      "3             740 390 590     Allahdino\n",
      "4         368 390 125 033     Allahdino\n",
      "...                   ...           ...\n",
      "2218          740 482 838  Mohenjo-daro\n",
      "2219          400 740 176  Mohenjo-daro\n",
      "2220          723 066 828  Mohenjo-daro\n",
      "2221      041 705 002 905  Mohenjo-daro\n",
      "2222  740 061 001 031 820       Harappa\n",
      "\n",
      "[2223 rows x 2 columns]\n",
      "train_rev df: \n",
      "                     reversed_text          site\n",
      "0                             013        Lothal\n",
      "1                         034 700       Harappa\n",
      "2                     740 407 590         Hulas\n",
      "3         740 405 590 806 002 820  Mohenjo-daro\n",
      "4     400 456 503 337 484 920 140  Mohenjo-daro\n",
      "...                           ...           ...\n",
      "1773              400 740 100 240       Harappa\n",
      "1774              400 740 032 840       Harappa\n",
      "1775              527 550 002 370  Mohenjo-daro\n",
      "1776              740 222 220 235  Mohenjo-daro\n",
      "1777      740 384 032 803 002 820  Mohenjo-daro\n",
      "\n",
      "[1778 rows x 2 columns]\n",
      "test rev df: \n",
      "                    reversed_text           site\n",
      "0                        390 003   Mohenjo-daro\n",
      "1                    520 240 235        Harappa\n",
      "2    740 405 590 220 003 002 861        Harappa\n",
      "3                            820        Harappa\n",
      "4    740 879 220 415 002 287 140   Mohenjo-daro\n",
      "..                           ...            ...\n",
      "440                      390 003     Kalibangan\n",
      "441              375 400 390 632  Chanhujo-daro\n",
      "442      090 740 255 705 002 861   Mohenjo-daro\n",
      "443                  400 740 176        Harappa\n",
      "444          740 016 002 920 317  Chanhujo-daro\n",
      "\n",
      "[445 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\" UnPickle the dataframes \"\"\"\n",
    "\n",
    "orig_sign_df = pd.read_pickle('pickle/orig_sign_df.pkl')\n",
    "print(\"Original sign df: \\n\", orig_sign_df)\n",
    "\n",
    "orig_df = pd.read_pickle('pickle/upd_orig_df.pkl')\n",
    "print(\"Updated original text df: \\n\", orig_df)\n",
    "\n",
    "df = pd.read_pickle('pickle/clean_df.pkl')\n",
    "print(\"Cleaned text df: \\n\", df )\n",
    "\n",
    "df_unclear = pd.read_pickle('pickle/unclear_df.pkl')\n",
    "print(\"Unclear text df: \\n\", df_unclear)\n",
    "\n",
    "df_multi_line = pd.read_pickle('pickle/multi_line_df.pkl')\n",
    "print(\"Multi-line text df: \\n\", df_multi_line)\n",
    "\n",
    "df_all_x = pd.read_pickle('pickle/all_x.pkl')\n",
    "df_all_y = pd.read_pickle('pickle/all_y.pkl')\n",
    "\n",
    "df_all_x_rev = pd.read_pickle('pickle/all_x_rev.pkl')\n",
    "df_all_y_rev = pd.read_pickle('pickle/all_y_rev.pkl')\n",
    "\n",
    "df_train_x = pd.read_pickle('pickle/train_x.pkl')\n",
    "df_train_y = pd.read_pickle('pickle/train_y.pkl')\n",
    "\n",
    "df_train_x_rev = pd.read_pickle('pickle/train_x_rev.pkl')\n",
    "df_train_y_rev = pd.read_pickle('pickle/train_y_rev.pkl')\n",
    "\n",
    "df_test_x = pd.read_pickle('pickle/test_x.pkl')\n",
    "df_test_y= pd.read_pickle('pickle/test_y.pkl')\n",
    "\n",
    "df_test_x_rev = pd.read_pickle('pickle/test_x_rev.pkl')\n",
    "df_test_y_rev = pd.read_pickle('pickle/test_y_rev.pkl')\n",
    "\n",
    "df_all = pd.read_pickle('pickle/all_df.pkl')\n",
    "print(\"all df: \\n\", df_all)\n",
    "\n",
    "df_train = pd.read_pickle('pickle/train_df.pkl')\n",
    "print(\"train df: \\n\", df_train)\n",
    "\n",
    "df_test = pd.read_pickle('pickle/test_df.pkl')\n",
    "print(\"test df: \\n\", df_test)\n",
    "\n",
    "df_all_rev = pd.read_pickle('pickle/all_rev_df.pkl')\n",
    "print(\"all_rev df: \\n\", df_all_rev)\n",
    "\n",
    "df_train_rev = pd.read_pickle('pickle/train_rev_df.pkl')\n",
    "print(\"train_rev df: \\n\", df_train_rev)\n",
    "\n",
    "df_test_rev = pd.read_pickle('pickle/test_rev_df.pkl')\n",
    "print(\"test rev df: \\n\", df_test_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f973c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core train df: \n",
      "                                   l_to_r_text direction icit_id          site\n",
      "0                 513 460 036 861 002 005 390       R/L    2548  Mohenjo-daro\n",
      "1                                 176 100 740       R/L    1213       Harappa\n",
      "2                                     003 156       R/L    2307  Mohenjo-daro\n",
      "3                                     220 520       R/L     282       Harappa\n",
      "4                                 140 706 064       R/L    1194       Harappa\n",
      "...                                       ...       ...     ...           ...\n",
      "1765  920 455 220 002 803 706 033 017 585 740       R/L    1293       Harappa\n",
      "1766                      740 690 435 255 055       L/R     570       Harappa\n",
      "1767              920 060 741 031 017 575 740       R/L    3679  Mohenjo-daro\n",
      "1768                      904 032 597 142 617       R/L    3779  Mohenjo-daro\n",
      "1769                  505 001 032 090 031 151       R/L    3301  Mohenjo-daro\n",
      "\n",
      "[1770 rows x 4 columns]\n",
      "core test df: \n",
      "                              l_to_r_text direction icit_id          site\n",
      "0                    798 233 790 900 740       R/L    2648  Mohenjo-daro\n",
      "1                861 368 001 803 235 520       R/L    3219  Mohenjo-daro\n",
      "2                        244 065 880 820       R/L    2535  Mohenjo-daro\n",
      "3                                005 390       R/L    2472  Mohenjo-daro\n",
      "4                            235 803 740       R/L    2476  Mohenjo-daro\n",
      "..                                   ...       ...     ...           ...\n",
      "438                          032 854 400       R/L     679       Harappa\n",
      "439  861 002 798 415 220 033 033 760 740       R/L    3711  Mohenjo-daro\n",
      "440                              031 156       R/L    1493       Harappa\n",
      "441                          003 231 740       R/L    1332       Harappa\n",
      "442                              175 520       R/L    2319  Mohenjo-daro\n",
      "\n",
      "[443 rows x 4 columns]\n",
      "core train_rev df: \n",
      "                                 reversed_text          site\n",
      "0                 390 005 002 861 036 460 513  Mohenjo-daro\n",
      "1                                 740 100 176       Harappa\n",
      "2                                     156 003  Mohenjo-daro\n",
      "3                                     520 220       Harappa\n",
      "4                                 064 706 140       Harappa\n",
      "...                                       ...           ...\n",
      "1765  740 585 017 033 706 803 002 220 455 920       Harappa\n",
      "1766                      055 255 435 690 740       Harappa\n",
      "1767              740 575 017 031 741 060 920  Mohenjo-daro\n",
      "1768                      617 142 597 032 904  Mohenjo-daro\n",
      "1769                  151 031 090 032 001 505  Mohenjo-daro\n",
      "\n",
      "[1770 rows x 2 columns]\n",
      "core test rev df: \n",
      "                            reversed_text          site\n",
      "0                    740 900 790 233 798  Mohenjo-daro\n",
      "1                520 235 803 001 368 861  Mohenjo-daro\n",
      "2                        820 880 065 244  Mohenjo-daro\n",
      "3                                390 005  Mohenjo-daro\n",
      "4                            740 803 235  Mohenjo-daro\n",
      "..                                   ...           ...\n",
      "438                          400 854 032       Harappa\n",
      "439  740 760 033 033 220 415 798 002 861  Mohenjo-daro\n",
      "440                              156 031       Harappa\n",
      "441                          740 231 003       Harappa\n",
      "442                              520 175  Mohenjo-daro\n",
      "\n",
      "[443 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\" UnPickle the Indus Core dataframes \"\"\"\n",
    "\n",
    "df_core_x = pd.read_pickle('pickle/core_x.pkl')\n",
    "df_core_y = pd.read_pickle('pickle/core_y.pkl')\n",
    "\n",
    "df_core_x_rev = pd.read_pickle('pickle/core_x_rev.pkl')\n",
    "df_core_y_rev = pd.read_pickle('pickle/core_y_rev.pkl')\n",
    "\n",
    "df_core_train_x = pd.read_pickle('pickle/core_train_x.pkl')\n",
    "df_core_train_y = pd.read_pickle('pickle/core_train_y.pkl')\n",
    "\n",
    "df_core_train_x_rev = pd.read_pickle('pickle/core_train_x_rev.pkl')\n",
    "df_core_train_y_rev = pd.read_pickle('pickle/core_train_y_rev.pkl')\n",
    "\n",
    "df_core_test_x = pd.read_pickle('pickle/core_test_x.pkl')\n",
    "df_core_test_y= pd.read_pickle('pickle/core_test_y.pkl')\n",
    "\n",
    "df_core_test_x_rev = pd.read_pickle('pickle/core_test_x_rev.pkl')\n",
    "df_core_test_y_rev = pd.read_pickle('pickle/core_test_y_rev.pkl')\n",
    "\n",
    "\n",
    "df_core_train = pd.read_pickle('pickle/core_train_df.pkl')\n",
    "print(\"core train df: \\n\", df_core_train)\n",
    "\n",
    "df_core_test = pd.read_pickle('pickle/core_test_df.pkl')\n",
    "print(\"core test df: \\n\", df_core_test)\n",
    "\n",
    "\n",
    "df_core_train_rev = pd.read_pickle('pickle/core_train_rev_df.pkl')\n",
    "print(\"core train_rev df: \\n\", df_core_train_rev)\n",
    "\n",
    "df_core_test_rev = pd.read_pickle('pickle/core_test_rev_df.pkl')\n",
    "print(\"core test rev df: \\n\", df_core_test_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f34d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_text(text):\n",
    "    # first split the string into chars\n",
    "    chars = text.split(' ')\n",
    "\n",
    "    # then reverse the split string list and join with a space\n",
    "    reversed_text = ' '.join(reversed(chars))\n",
    "    return reversed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f265a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ngram as list given a text (pass direction_of_string as \"R/L\" or \"L/R\")\n",
    "def get_ngrams_as_list(data,direction_of_string,num):\n",
    "    \n",
    "    if(direction_of_string==\"R/L\"):\n",
    "        # We need to convert R/L text to L/R to be able to get ngrams using nltk\n",
    "        data_string = reverse_text(data)\n",
    "    \n",
    "    else: data_string= data\n",
    "    \n",
    "    n_grams =  ngrams(nltk.word_tokenize(data_string), num)\n",
    "    return  [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e406f",
   "metadata": {},
   "source": [
    "## n-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da414ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tokenize the text\n",
    "\n",
    "If we need to generate ngrams from it from r to l text, ngrams would be in opposite direction, so\n",
    "use reversed text to generate tokenized_text (l to r) and regular text to generate reverse_tokenized_text (r to l)\n",
    "\"\"\"   \n",
    "\"\"\"For all data\"\"\"\n",
    "tokenized_text_all = list(df_all_x[df_all_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "reverse_tokenized_text_all = list(df_all_x_rev[df_all_x_rev.reversed_text!=''].reversed_text.apply(word_tokenize))\n",
    "\n",
    "\n",
    "\"\"\"For Train data\"\"\"\n",
    "tokenized_text = list(df_train_x[df_train_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "reverse_tokenized_text = list(df_train_x_rev[df_train_x_rev.reversed_text!=''].reversed_text.apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff74723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"For Indus Core Region All data\"\"\"\n",
    "all_core_tokenized_text = list(df_core_x[df_core_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "all_core_reverse_tokenized_text = list(df_core_x_rev[df_core_x_rev.reversed_text!=''].reversed_text.apply(word_tokenize))\n",
    "\n",
    "\n",
    "\"\"\"For Indus Core Region Train data\"\"\"\n",
    "core_tokenized_text = list(df_core_train_x[df_core_train_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "core_reverse_tokenized_text = list(df_core_train_x_rev[df_core_train_x_rev.reversed_text!=''].reversed_text.apply(word_tokenize))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba770861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess the tokenized text for n-grams language modeling\n",
    "Do this for all data an train data\n",
    "\"\"\"\n",
    "import array as arr\n",
    "model_name_list = [\"MLE\",\"KneserNeyInterpolated\", \"Laplace\", \"Lidstone\",\"StupidBackoff\", \"WittenBellInterpolated\"]\n",
    "\n",
    "all_data_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_unigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_unigram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_unigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_unigram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_bigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_bigram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_bigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_bigram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_trigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_trigram  = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_trigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_trigram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_septagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_septagram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_septagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_septagram = [None,None, None, None, None,None]\n",
    "\n",
    "train_data_rev_list = [None,None, None, None, None,None]\n",
    "padded_sents_rev_list = [None,None, None, None, None,None]\n",
    "\n",
    "\n",
    "for index in range (0,6):\n",
    "\n",
    "    all_data_list_fwd_unigram[index], all_padded_sents_list_fwd_unigram[index] = padded_everygram_pipeline(1, tokenized_text)\n",
    "    all_data_list_rev_unigram[index], all_padded_sents_list_rev_unigram[index] = padded_everygram_pipeline(1, reverse_tokenized_text)\n",
    "    train_data_list_fwd_unigram[index], padded_sents_list_fwd_unigram[index] = padded_everygram_pipeline(1, tokenized_text)\n",
    "    train_data_list_rev_unigram[index], padded_sents_list_rev_unigram[index] = padded_everygram_pipeline(1, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_bigram[index], all_padded_sents_list_fwd_bigram[index] = padded_everygram_pipeline(2, tokenized_text)\n",
    "    all_data_list_rev_bigram[index], all_padded_sents_list_rev_bigram[index] = padded_everygram_pipeline(2, reverse_tokenized_text)\n",
    "    train_data_list_fwd_bigram[index], padded_sents_list_fwd_bigram[index] = padded_everygram_pipeline(2, tokenized_text)\n",
    "    train_data_list_rev_bigram[index], padded_sents_list_rev_bigram[index] = padded_everygram_pipeline(2, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_trigram[index], all_padded_sents_list_fwd_trigram[index] = padded_everygram_pipeline(3, tokenized_text)\n",
    "    all_data_list_rev_trigram[index], all_padded_sents_list_rev_trigram[index] = padded_everygram_pipeline(3, reverse_tokenized_text)\n",
    "    train_data_list_fwd_trigram[index], padded_sents_list_fwd_trigram[index] = padded_everygram_pipeline(3, tokenized_text)\n",
    "    train_data_list_rev_trigram[index], padded_sents_list_rev_trigram[index] = padded_everygram_pipeline(3, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_quadgram[index], all_padded_sents_list_fwd_quadgram[index] = padded_everygram_pipeline(4, tokenized_text)\n",
    "    all_data_list_rev_quadgram[index], all_padded_sents_list_rev_quadgram[index] = padded_everygram_pipeline(4, reverse_tokenized_text)\n",
    "    train_data_list_fwd_quadgram[index], padded_sents_list_fwd_quadgram[index] = padded_everygram_pipeline(4, tokenized_text)\n",
    "    train_data_list_rev_quadgram[index], padded_sents_list_rev_quadgram[index] = padded_everygram_pipeline(4, reverse_tokenized_text)\n",
    "\n",
    "    all_data_list_fwd_pentagram[index], all_padded_sents_list_fwd_pentagram[index] = padded_everygram_pipeline(5, tokenized_text)\n",
    "    all_data_list_rev_pentagram[index], all_padded_sents_list_rev_pentagram[index] = padded_everygram_pipeline(5, reverse_tokenized_text)\n",
    "    train_data_list_fwd_pentagram[index], padded_sents_list_fwd_pentagram[index] = padded_everygram_pipeline(5, tokenized_text)\n",
    "    train_data_list_rev_pentagram[index], padded_sents_list_rev_pentagram[index] = padded_everygram_pipeline(5, reverse_tokenized_text)\n",
    "\n",
    "    all_data_list_fwd_hexagram[index], all_padded_sents_list_fwd_hexagram[index] = padded_everygram_pipeline(6, tokenized_text)\n",
    "    all_data_list_rev_hexagram[index], all_padded_sents_list_rev_hexagram[index] = padded_everygram_pipeline(6, reverse_tokenized_text)\n",
    "    train_data_list_fwd_hexagram[index], padded_sents_list_fwd_hexagram[index] = padded_everygram_pipeline(6, tokenized_text)\n",
    "    train_data_list_rev_hexagram[index], padded_sents_list_rev_hexagram[index] = padded_everygram_pipeline(6, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_septagram[index], all_padded_sents_list_fwd_septagram[index] = padded_everygram_pipeline(7, tokenized_text)\n",
    "    all_data_list_rev_septagram[index], all_padded_sents_list_rev_septagram[index] = padded_everygram_pipeline(7, reverse_tokenized_text)\n",
    "    train_data_list_fwd_septagram[index], padded_sents_list_fwd_septagram[index] = padded_everygram_pipeline(7, tokenized_text)\n",
    "    train_data_list_rev_septagram[index], padded_sents_list_rev_septagram[index] = padded_everygram_pipeline(7, reverse_tokenized_text)\n",
    "\n",
    "    \n",
    "print_train_data_details= False\n",
    "#If you iterate through this, the iterator is done with and model \n",
    "# fitting won't work subsequently\n",
    "# so set print_train_data_details= False before trying the actual model\n",
    "\n",
    "# Example\n",
    "if(print_train_data_details):\n",
    "    for ngramlize_sent in train_data_list_fwd_quadgram[0]:\n",
    "        print(list(ngramlize_sent))\n",
    "        print()\n",
    "    print('#############')\n",
    "    list(padded_sents_list_fwd_quadgram[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff1f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For All data: Unigram, Bigram, Trigram, Quadgram, Pentagram, Hexagram Models for both fwd text and reverse tex with the following\n",
    "models. Ignoring AbsoluteDiscountingInterpolated model\n",
    "\"\"\"\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import AbsoluteDiscountingInterpolated\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.models import Lidstone\n",
    "from nltk.lm.models import StupidBackoff\n",
    "from nltk.lm.models import WittenBellInterpolated\n",
    "\n",
    "gamma=0.75\n",
    "\n",
    "model_MLE_list_fwd_all = []\n",
    "model_KneserNeyInterpolated_list_fwd_all = []\n",
    "model_Laplace_list_fwd_all = []\n",
    "model_Lidstone_list_fwd_all = []\n",
    "model_StupidBackoff_list_fwd_all = []\n",
    "model_WittenBellInterpolated_list_fwd_all = []\n",
    "\n",
    "\n",
    "model_MLE_list_rev_all = []\n",
    "model_KneserNeyInterpolated_list_rev_all = []\n",
    "model_Laplace_list_rev_all = []\n",
    "model_Lidstone_list_rev_all = []\n",
    "model_StupidBackoff_list_rev_all = []\n",
    "model_WittenBellInterpolated_list_rev_all= []\n",
    "\n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_fwd_all.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_fwd_all.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_fwd_all.append(Laplace(index))\n",
    "    model_Lidstone_list_fwd_all.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_fwd_all.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_fwd_all.append(WittenBellInterpolated(index))\n",
    "    \n",
    "    \n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_rev_all.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_rev_all.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_rev_all.append(Laplace(index))\n",
    "    model_Lidstone_list_rev_all.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_rev_all.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_rev_all.append(WittenBellInterpolated(index))\n",
    "    \n",
    "models_list_fwd_unigram_all = [model_MLE_list_fwd_all[0] ,model_KneserNeyInterpolated_list_fwd_all[0] ,model_Laplace_list_fwd_all[0] , model_Lidstone_list_fwd_all[0] , model_StupidBackoff_list_fwd_all[0],model_WittenBellInterpolated_list_fwd_all[0]]\n",
    "models_list_rev_unigram_all = [model_MLE_list_rev_all[0] ,model_KneserNeyInterpolated_list_rev_all[0] ,model_Laplace_list_rev_all[0] , model_Lidstone_list_rev_all[0] , model_StupidBackoff_list_rev_all[0], model_WittenBellInterpolated_list_rev_all[0]]\n",
    "\n",
    "models_list_fwd_bigram_all = [model_MLE_list_fwd_all[1] ,model_KneserNeyInterpolated_list_fwd_all[1] ,model_Laplace_list_fwd_all[1] , model_Lidstone_list_fwd_all[1] , model_StupidBackoff_list_fwd_all[1],model_WittenBellInterpolated_list_fwd_all[1]]\n",
    "models_list_rev_bigram_all = [model_MLE_list_rev_all[1] ,model_KneserNeyInterpolated_list_rev_all[1] ,model_Laplace_list_rev_all[1] , model_Lidstone_list_rev_all[1] , model_StupidBackoff_list_rev_all[1], model_WittenBellInterpolated_list_rev_all[1]]\n",
    "\n",
    "models_list_fwd_trigram_all = [model_MLE_list_fwd_all[2] ,model_KneserNeyInterpolated_list_fwd_all[2] ,model_Laplace_list_fwd_all[2] , model_Lidstone_list_fwd_all[2] , model_StupidBackoff_list_fwd_all[2],model_WittenBellInterpolated_list_fwd_all[2]]\n",
    "models_list_rev_trigram_all = [model_MLE_list_rev_all[2] ,model_KneserNeyInterpolated_list_rev_all[2] ,model_Laplace_list_rev_all[2] , model_Lidstone_list_rev_all[2] , model_StupidBackoff_list_rev_all[2],model_WittenBellInterpolated_list_rev_all[2]]\n",
    "\n",
    "models_list_fwd_quadgram_all = [model_MLE_list_fwd_all[3] ,model_KneserNeyInterpolated_list_fwd_all[3] ,model_Laplace_list_fwd_all[3] , model_Lidstone_list_fwd_all[3] , model_StupidBackoff_list_fwd_all[3],model_WittenBellInterpolated_list_fwd_all[3]]\n",
    "models_list_rev_quadgram_all = [model_MLE_list_rev_all[3] ,model_KneserNeyInterpolated_list_rev_all[3] ,model_Laplace_list_rev_all[3] , model_Lidstone_list_rev_all[3] , model_StupidBackoff_list_rev_all[3],model_WittenBellInterpolated_list_rev_all[3]]\n",
    "\n",
    "models_list_fwd_pentagram_all = [model_MLE_list_fwd_all[4] ,model_KneserNeyInterpolated_list_fwd_all[4] ,model_Laplace_list_fwd_all[4] , model_Lidstone_list_fwd_all[4] , model_StupidBackoff_list_fwd_all[4],model_WittenBellInterpolated_list_fwd_all[4]]\n",
    "models_list_rev_pentagram_all = [model_MLE_list_rev_all[4] ,model_KneserNeyInterpolated_list_rev_all[4] ,model_Laplace_list_rev_all[4] , model_Lidstone_list_rev_all[4] , model_StupidBackoff_list_rev_all[4],model_WittenBellInterpolated_list_rev_all[4]]\n",
    "\n",
    "models_list_fwd_hexagram_all = [model_MLE_list_fwd_all[5] ,model_KneserNeyInterpolated_list_fwd_all[5] ,model_Laplace_list_fwd_all[5] , model_Lidstone_list_fwd_all[5] , model_StupidBackoff_list_fwd_all[5],model_WittenBellInterpolated_list_fwd_all[5]]\n",
    "models_list_rev_hexagram_all = [model_MLE_list_rev_all[5] ,model_KneserNeyInterpolated_list_rev_all[5] ,model_Laplace_list_rev_all[5] , model_Lidstone_list_rev_all[5] , model_StupidBackoff_list_rev_all[5],model_WittenBellInterpolated_list_rev_all[5]]\n",
    "\n",
    "models_list_fwd_septagram_all = [model_MLE_list_fwd_all[6] ,model_KneserNeyInterpolated_list_fwd_all[6] ,model_Laplace_list_fwd_all[6] , model_Lidstone_list_fwd_all[6] , model_StupidBackoff_list_fwd_all[6],model_WittenBellInterpolated_list_fwd_all[6]]\n",
    "models_list_rev_septagram_all = [model_MLE_list_rev_all[6] ,model_KneserNeyInterpolated_list_rev_all[6] ,model_Laplace_list_rev_all[6] , model_Lidstone_list_rev_all[6] , model_StupidBackoff_list_rev_all[6],model_WittenBellInterpolated_list_rev_all[6]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3da5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For Train data: Unigram, Bigram, Trigram, Quadgram, Pentagram, Hexagram Models for both fwd text and reverse tex with the following\n",
    "models. Ignoring AbsoluteDiscountingInterpolated model\n",
    "\"\"\"\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import AbsoluteDiscountingInterpolated\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.models import Lidstone\n",
    "from nltk.lm.models import StupidBackoff\n",
    "from nltk.lm.models import WittenBellInterpolated\n",
    "\n",
    "gamma=0.75\n",
    "\n",
    "model_MLE_list_fwd = []\n",
    "model_KneserNeyInterpolated_list_fwd = []\n",
    "model_Laplace_list_fwd = []\n",
    "model_Lidstone_list_fwd = []\n",
    "model_StupidBackoff_list_fwd = []\n",
    "model_WittenBellInterpolated_list_fwd= []\n",
    "\n",
    "\n",
    "model_MLE_list_rev = []\n",
    "model_KneserNeyInterpolated_list_rev = []\n",
    "model_Laplace_list_rev = []\n",
    "model_Lidstone_list_rev = []\n",
    "model_StupidBackoff_list_rev = []\n",
    "model_WittenBellInterpolated_list_rev= []\n",
    "\n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_fwd.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_fwd.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_fwd.append(Laplace(index))\n",
    "    model_Lidstone_list_fwd.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_fwd.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_fwd.append(WittenBellInterpolated(index))\n",
    "    \n",
    "    \n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_rev.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_rev.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_rev.append(Laplace(index))\n",
    "    model_Lidstone_list_rev.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_rev.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_rev.append(WittenBellInterpolated(index))\n",
    "    \n",
    "models_list_fwd_unigram = [model_MLE_list_fwd[0] ,model_KneserNeyInterpolated_list_fwd[0] ,model_Laplace_list_fwd[0] , model_Lidstone_list_fwd[0] , model_StupidBackoff_list_fwd[0],model_WittenBellInterpolated_list_fwd[0]]\n",
    "models_list_rev_unigram = [model_MLE_list_rev[0] ,model_KneserNeyInterpolated_list_rev[0] ,model_Laplace_list_rev[0] , model_Lidstone_list_rev[0] , model_StupidBackoff_list_rev[0], model_WittenBellInterpolated_list_rev[0]]\n",
    "\n",
    "models_list_fwd_bigram = [model_MLE_list_fwd[1] ,model_KneserNeyInterpolated_list_fwd[1] ,model_Laplace_list_fwd[1] , model_Lidstone_list_fwd[1] , model_StupidBackoff_list_fwd[1],model_WittenBellInterpolated_list_fwd[1]]\n",
    "models_list_rev_bigram = [model_MLE_list_rev[1] ,model_KneserNeyInterpolated_list_rev[1] ,model_Laplace_list_rev[1] , model_Lidstone_list_rev[1] , model_StupidBackoff_list_rev[1], model_WittenBellInterpolated_list_rev[1]]\n",
    "\n",
    "models_list_fwd_trigram = [model_MLE_list_fwd[2] ,model_KneserNeyInterpolated_list_fwd[2] ,model_Laplace_list_fwd[2] , model_Lidstone_list_fwd[2] , model_StupidBackoff_list_fwd[2],model_WittenBellInterpolated_list_fwd[2]]\n",
    "models_list_rev_trigram = [model_MLE_list_rev[2] ,model_KneserNeyInterpolated_list_rev[2] ,model_Laplace_list_rev[2] , model_Lidstone_list_rev[2] , model_StupidBackoff_list_rev[2],model_WittenBellInterpolated_list_rev[2]]\n",
    "\n",
    "models_list_fwd_quadgram = [model_MLE_list_fwd[3] ,model_KneserNeyInterpolated_list_fwd[3] ,model_Laplace_list_fwd[3] , model_Lidstone_list_fwd[3] , model_StupidBackoff_list_fwd[3],model_WittenBellInterpolated_list_fwd[3]]\n",
    "models_list_rev_quadgram = [model_MLE_list_rev[3] ,model_KneserNeyInterpolated_list_rev[3] ,model_Laplace_list_rev[3] , model_Lidstone_list_rev[3] , model_StupidBackoff_list_rev[3],model_WittenBellInterpolated_list_rev[3]]\n",
    "\n",
    "models_list_fwd_pentagram = [model_MLE_list_fwd[4] ,model_KneserNeyInterpolated_list_fwd[4] ,model_Laplace_list_fwd[4] , model_Lidstone_list_fwd[4] , model_StupidBackoff_list_fwd[4],model_WittenBellInterpolated_list_fwd[4]]\n",
    "models_list_rev_pentagram = [model_MLE_list_rev[4] ,model_KneserNeyInterpolated_list_rev[4] ,model_Laplace_list_rev[4] , model_Lidstone_list_rev[4] , model_StupidBackoff_list_rev[4],model_WittenBellInterpolated_list_rev[4]]\n",
    "\n",
    "models_list_fwd_hexagram = [model_MLE_list_fwd[5] ,model_KneserNeyInterpolated_list_fwd[5] ,model_Laplace_list_fwd[5] , model_Lidstone_list_fwd[5] , model_StupidBackoff_list_fwd[5],model_WittenBellInterpolated_list_fwd[5]]\n",
    "models_list_rev_hexagram = [model_MLE_list_rev[5] ,model_KneserNeyInterpolated_list_rev[5] ,model_Laplace_list_rev[5] , model_Lidstone_list_rev[5] , model_StupidBackoff_list_rev[5],model_WittenBellInterpolated_list_rev[5]]\n",
    "\n",
    "models_list_fwd_septagram = [model_MLE_list_fwd[6] ,model_KneserNeyInterpolated_list_fwd[6] ,model_Laplace_list_fwd[6] , model_Lidstone_list_fwd[6] , model_StupidBackoff_list_fwd[6],model_WittenBellInterpolated_list_fwd[6]]\n",
    "models_list_rev_septagram = [model_MLE_list_rev[6] ,model_KneserNeyInterpolated_list_rev[6] ,model_Laplace_list_rev[6] , model_Lidstone_list_rev[6] , model_StupidBackoff_list_rev[6],model_WittenBellInterpolated_list_rev[6]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02e83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_train_models(models_list,model_type, this_train_data_list,this_padded_sents_list):\n",
    "    for index in range (0,len(models_list)):\n",
    "        models_list[index].fit(this_train_data_list[index], this_padded_sents_list[index])\n",
    "        print(\"Fit:\",model_name_list[index], type, \"Order:\", models_list[index].order, models_list[index].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c387352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the models for All data\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the models for All data\")\n",
    "fit_and_train_models(models_list_fwd_unigram_all ,\"fwd\", all_data_list_fwd_unigram,all_padded_sents_list_fwd_unigram)\n",
    "fit_and_train_models(models_list_rev_unigram_all , \"rev\", all_data_list_rev_unigram,all_padded_sents_list_rev_unigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_bigram_all ,\"fwd\", all_data_list_fwd_bigram,all_padded_sents_list_fwd_bigram)\n",
    "fit_and_train_models(models_list_rev_bigram_all ,\"rev\", all_data_list_rev_bigram,all_padded_sents_list_rev_bigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_trigram_all ,\"fwd\", all_data_list_fwd_trigram,all_padded_sents_list_fwd_trigram)\n",
    "fit_and_train_models(models_list_rev_trigram_all , \"rev\",all_data_list_rev_trigram,all_padded_sents_list_rev_trigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_quadgram_all , \"fwd\",all_data_list_fwd_quadgram,all_padded_sents_list_fwd_quadgram)\n",
    "fit_and_train_models(models_list_rev_quadgram_all ,\"rev\", all_data_list_rev_quadgram,all_padded_sents_list_rev_quadgram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_pentagram_all ,\"fwd\", all_data_list_fwd_pentagram,all_padded_sents_list_fwd_pentagram)\n",
    "fit_and_train_models(models_list_rev_pentagram_all ,\"rev\", all_data_list_rev_pentagram,all_padded_sents_list_rev_pentagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_hexagram_all , \"fwd\",all_data_list_fwd_hexagram,all_padded_sents_list_fwd_hexagram)\n",
    "fit_and_train_models(models_list_rev_hexagram_all , \"rev\",all_data_list_rev_hexagram,all_padded_sents_list_rev_hexagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_septagram_all ,\"fwd\", all_data_list_fwd_septagram,all_padded_sents_list_fwd_septagram)\n",
    "fit_and_train_models(models_list_rev_septagram_all , \"rev\", all_data_list_rev_septagram,all_padded_sents_list_rev_septagram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c68df2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the models for Train data\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 550 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 552 items>\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the models for Train data\")\n",
    "\n",
    "fit_and_train_models(models_list_fwd_unigram ,\"fwd\", train_data_list_fwd_unigram,padded_sents_list_fwd_unigram)\n",
    "fit_and_train_models(models_list_rev_unigram , \"rev\", train_data_list_rev_unigram,padded_sents_list_rev_unigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_bigram ,\"fwd\", train_data_list_fwd_bigram,padded_sents_list_fwd_bigram)\n",
    "fit_and_train_models(models_list_rev_bigram ,\"rev\", train_data_list_rev_bigram,padded_sents_list_rev_bigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_trigram ,\"fwd\", train_data_list_fwd_trigram,padded_sents_list_fwd_trigram)\n",
    "fit_and_train_models(models_list_rev_trigram , \"rev\",train_data_list_rev_trigram,padded_sents_list_rev_trigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_quadgram , \"fwd\",train_data_list_fwd_quadgram,padded_sents_list_fwd_quadgram)\n",
    "fit_and_train_models(models_list_rev_quadgram ,\"rev\", train_data_list_rev_quadgram,padded_sents_list_rev_quadgram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_pentagram ,\"fwd\", train_data_list_fwd_pentagram,padded_sents_list_fwd_pentagram)\n",
    "fit_and_train_models(models_list_rev_pentagram ,\"rev\", train_data_list_rev_pentagram,padded_sents_list_rev_pentagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_hexagram , \"fwd\",train_data_list_fwd_hexagram,padded_sents_list_fwd_hexagram)\n",
    "fit_and_train_models(models_list_rev_hexagram , \"rev\",train_data_list_rev_hexagram,padded_sents_list_rev_hexagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_septagram ,\"fwd\", train_data_list_fwd_septagram,padded_sents_list_fwd_septagram)\n",
    "fit_and_train_models(models_list_rev_septagram , \"rev\", train_data_list_rev_septagram,padded_sents_list_rev_septagram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e5e4119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled pickle/MLE_fwd_all_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_1.pkl\n",
      "Pickled pickle/Laplace_fwd_all_1.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_1.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_1.pkl\n",
      "Pickled pickle/MLE_rev_all_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_1.pkl\n",
      "Pickled pickle/Laplace_rev_all_1.pkl\n",
      "Pickled pickle/Lidstone_rev_all_1.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_1.pkl\n",
      "Pickled pickle/MLE_fwd_all_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_2.pkl\n",
      "Pickled pickle/Laplace_fwd_all_2.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_2.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_2.pkl\n",
      "Pickled pickle/MLE_rev_all_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_2.pkl\n",
      "Pickled pickle/Laplace_rev_all_2.pkl\n",
      "Pickled pickle/Lidstone_rev_all_2.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_2.pkl\n",
      "Pickled pickle/MLE_fwd_all_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_3.pkl\n",
      "Pickled pickle/Laplace_fwd_all_3.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_3.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_3.pkl\n",
      "Pickled pickle/MLE_rev_all_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_3.pkl\n",
      "Pickled pickle/Laplace_rev_all_3.pkl\n",
      "Pickled pickle/Lidstone_rev_all_3.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_3.pkl\n",
      "Pickled pickle/MLE_fwd_all_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_4.pkl\n",
      "Pickled pickle/Laplace_fwd_all_4.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_4.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_4.pkl\n",
      "Pickled pickle/MLE_rev_all_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_4.pkl\n",
      "Pickled pickle/Laplace_rev_all_4.pkl\n",
      "Pickled pickle/Lidstone_rev_all_4.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_4.pkl\n",
      "Pickled pickle/MLE_fwd_all_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_5.pkl\n",
      "Pickled pickle/Laplace_fwd_all_5.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_5.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_5.pkl\n",
      "Pickled pickle/MLE_rev_all_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_5.pkl\n",
      "Pickled pickle/Laplace_rev_all_5.pkl\n",
      "Pickled pickle/Lidstone_rev_all_5.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_5.pkl\n",
      "Pickled pickle/MLE_fwd_all_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_6.pkl\n",
      "Pickled pickle/Laplace_fwd_all_6.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_6.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_6.pkl\n",
      "Pickled pickle/MLE_rev_all_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_6.pkl\n",
      "Pickled pickle/Laplace_rev_all_6.pkl\n",
      "Pickled pickle/Lidstone_rev_all_6.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_6.pkl\n",
      "Pickled pickle/MLE_fwd_all_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_7.pkl\n",
      "Pickled pickle/Laplace_fwd_all_7.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_7.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_7.pkl\n",
      "Pickled pickle/MLE_rev_all_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_7.pkl\n",
      "Pickled pickle/Laplace_rev_all_7.pkl\n",
      "Pickled pickle/Lidstone_rev_all_7.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_7.pkl\n",
      "Pickled pickle/MLE_fwd_train_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_1.pkl\n",
      "Pickled pickle/Laplace_fwd_train_1.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_1.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_1.pkl\n",
      "Pickled pickle/MLE_rev_train_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_1.pkl\n",
      "Pickled pickle/Laplace_rev_train_1.pkl\n",
      "Pickled pickle/Lidstone_rev_train_1.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_1.pkl\n",
      "Pickled pickle/MLE_fwd_train_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_2.pkl\n",
      "Pickled pickle/Laplace_fwd_train_2.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_2.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_2.pkl\n",
      "Pickled pickle/MLE_rev_train_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_2.pkl\n",
      "Pickled pickle/Laplace_rev_train_2.pkl\n",
      "Pickled pickle/Lidstone_rev_train_2.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_2.pkl\n",
      "Pickled pickle/MLE_fwd_train_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_3.pkl\n",
      "Pickled pickle/Laplace_fwd_train_3.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_3.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_3.pkl\n",
      "Pickled pickle/MLE_rev_train_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_3.pkl\n",
      "Pickled pickle/Laplace_rev_train_3.pkl\n",
      "Pickled pickle/Lidstone_rev_train_3.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_3.pkl\n",
      "Pickled pickle/MLE_fwd_train_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_4.pkl\n",
      "Pickled pickle/Laplace_fwd_train_4.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_4.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_4.pkl\n",
      "Pickled pickle/MLE_rev_train_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_4.pkl\n",
      "Pickled pickle/Laplace_rev_train_4.pkl\n",
      "Pickled pickle/Lidstone_rev_train_4.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_4.pkl\n",
      "Pickled pickle/MLE_fwd_train_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_5.pkl\n",
      "Pickled pickle/Laplace_fwd_train_5.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_5.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_5.pkl\n",
      "Pickled pickle/MLE_rev_train_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_5.pkl\n",
      "Pickled pickle/Laplace_rev_train_5.pkl\n",
      "Pickled pickle/Lidstone_rev_train_5.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_5.pkl\n",
      "Pickled pickle/MLE_fwd_train_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_6.pkl\n",
      "Pickled pickle/Laplace_fwd_train_6.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_6.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_6.pkl\n",
      "Pickled pickle/MLE_rev_train_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_6.pkl\n",
      "Pickled pickle/Laplace_rev_train_6.pkl\n",
      "Pickled pickle/Lidstone_rev_train_6.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_6.pkl\n",
      "Pickled pickle/MLE_fwd_train_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_7.pkl\n",
      "Pickled pickle/Laplace_fwd_train_7.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_7.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_7.pkl\n",
      "Pickled pickle/MLE_rev_train_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_7.pkl\n",
      "Pickled pickle/Laplace_rev_train_7.pkl\n",
      "Pickled pickle/Lidstone_rev_train_7.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_7.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pickle all the trained Language Models\"\"\"\n",
    "def pickle_models(models_list, model_type, data_type):\n",
    "    for index in range (0,len(models_list)):\n",
    "        file_name = \"pickle/\" + model_name_list[index]+ \"_\" + model_type + \"_\" + data_type + \"_\" + str(models_list[index].order) +\".pkl\"\n",
    "        pickle.dump(models_list[index],open(file_name, 'wb'))\n",
    "        print(\"Pickled\",file_name)\n",
    "\n",
    "pickle_models(models_list_fwd_unigram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_unigram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_bigram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_bigram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_trigram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_trigram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_quadgram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_quadgram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_pentagram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_pentagram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_hexagram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_hexagram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_septagram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_septagram_all,\"rev\",\"all\")\n",
    "\n",
    "\n",
    "pickle_models(models_list_fwd_unigram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_unigram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_bigram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_bigram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_trigram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_trigram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_quadgram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_quadgram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_pentagram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_pentagram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_hexagram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_hexagram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_septagram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_septagram,\"rev\",\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730f194",
   "metadata": {},
   "source": [
    "## Initial Terminal Character Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff608753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 414 items>\n",
      "<NgramCounter with 2 ngram orders and 11697 ngrams>\n",
      "231\n",
      "514\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Build Model for relationship between Initial and Terminal characters\n",
    "This can be a bigram model. Pick a reasonably good model\n",
    "Remove all characters other than initial and terminal and then tokenize\n",
    "\"\"\"\n",
    "tokenized_text_temp = list(df_train_x[df_train_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "\n",
    "      \n",
    "tokenized_text_it = []\n",
    "for i in range(len(tokenized_text_temp)):\n",
    "    \n",
    "    l= tokenized_text_temp[i]\n",
    "    #single character text, ignore it\n",
    "    if(len(l)>1):\n",
    "        del l[1:len(l)-1]\n",
    "        l[0],l[1] = l[1], l[0]  #swap\n",
    "        tokenized_text_it.append(l)\n",
    "\n",
    "\"\"\" Instantiate the model\"\"\"\n",
    "k=2\n",
    "model_it_bigram_kn = KneserNeyInterpolated(k) #Bigram model\n",
    "train_data_it, padded_sents_it = padded_everygram_pipeline(k, tokenized_text_it)\n",
    "\n",
    "\n",
    "print_train_data_details= False\n",
    "#If you iterate through this, the iterator is done with and model \n",
    "# fitting won't work subsequently\n",
    "# so set print_train_data_details= False before trying the actual model\n",
    "\n",
    "if(print_train_data_details):\n",
    "    for ngramlize_sent in train_data_it:\n",
    "        print(list(ngramlize_sent))\n",
    "        print()\n",
    "    print('#############')\n",
    "    list(padded_sents_it)\n",
    "    \n",
    "model_it_bigram_kn.fit(train_data_it, padded_sents_it)\n",
    "    \n",
    "print(model_it_bigram_kn.vocab)\n",
    "print(model_it_bigram_kn.counts)\n",
    "print(model_it_bigram_kn.generate(1, ['804'], 8))\n",
    "print(model_it_bigram_kn.generate(1, ['621'], 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d6076f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled KneserNeyInterpolated_it_2.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pickle this model\"\"\"\n",
    "pickle.dump(model_it_bigram_kn,open(\"pickle/KneserNeyInterpolated_it_2.pkl\", 'wb'))\n",
    "print(\"Pickled\",\"KneserNeyInterpolated_it_2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cf2c6",
   "metadata": {},
   "source": [
    "## Models for perplexity for Indus Core and Non Core region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6a1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_and_save_model(model_name, model_order, gamma, data_type, tokenized_text, reverse_tokenized_text):\n",
    "      \n",
    "    if(model_name==\"MLE\"):\n",
    "        model_fwd = MLE(model_order)\n",
    "        model_rev = MLE(model_order)\n",
    "    elif(model_name==\"KneserNeyInterpolated\"):\n",
    "        model_fwd = KneserNeyInterpolated(model_order)\n",
    "        model_rev = KneserNeyInterpolated(model_order)\n",
    "    elif(model_name==\"Laplace\"):\n",
    "        model_fwd = Laplace(model_order)\n",
    "        model_rev = Laplace(model_order)\n",
    "    elif(model_name==\"Lidstone\"):\n",
    "        model_fwd = Lidstone(gamma, model_order) \n",
    "        model_rev = Lidstone(gamma, model_order)\n",
    "    elif(model_name==\"StupidBackoff\"):\n",
    "        model_fwd = StupidBackoff(model_order,model_order) \n",
    "        model_rev = StupidBackoff(model_order, model_order)\n",
    "    elif(model_name==\"WittenBellInterpolated\"):\n",
    "        model_fwd = WittenBellInterpolated(model_order) \n",
    "        model_rev = WittenBellInterpolated(model_order)\n",
    "    \n",
    "\n",
    "    fwd_data, padded_fwd_sents = padded_everygram_pipeline(model_order, tokenized_text)\n",
    "    rev_data, padded_rev_sents = padded_everygram_pipeline(model_order, reverse_tokenized_text)\n",
    "\n",
    "\n",
    "    print_data_details= False\n",
    "    #If you iterate through this, the iterator is done with and model \n",
    "    # fitting won't work subsequently\n",
    "    # so set print_train_data_details= False before trying the actual model\n",
    "\n",
    "    if(print_data_details):\n",
    "        for ngramlize_sent in fwd_data:\n",
    "            print(list(ngramlize_sent))\n",
    "            print()\n",
    "        print('#############')\n",
    "        list(padded_fwd_sents)\n",
    "\n",
    "    model_fwd.fit(fwd_data, padded_fwd_sents)\n",
    "    model_rev.fit(rev_data, padded_rev_sents)\n",
    "    \n",
    "    \"\"\"Pickle Model\"\"\"\n",
    "    direction= \"fwd\"\n",
    "    name = \"pickle/\"+ model_name + \"_\" + direction + \"_\" + data_type + \"_\"+ str(model_order) + \".pkl\"\n",
    "    pickle.dump(model_fwd,open(name, 'wb'))\n",
    "    print(\"Pickled\",name)\n",
    "\n",
    "    direction = \"rev\"\n",
    "    name = \"pickle/\"+ model_name + \"_\" + direction + \"_\" + data_type + \"_\"+ str(model_order) + \".pkl\"\n",
    "    pickle.dump(model_rev,open(name, 'wb'))\n",
    "    print(\"Pickled\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5124336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled pickle/MLE_fwd_core_train_2.pkl\n",
      "Pickled pickle/MLE_rev_core_train_2.pkl\n",
      "Pickled pickle/MLE_fwd_core_train_3.pkl\n",
      "Pickled pickle/MLE_rev_core_train_3.pkl\n",
      "Pickled pickle/MLE_fwd_core_train_4.pkl\n",
      "Pickled pickle/MLE_rev_core_train_4.pkl\n",
      "Pickled pickle/MLE_fwd_core_train_5.pkl\n",
      "Pickled pickle/MLE_rev_core_train_5.pkl\n",
      "Pickled pickle/MLE_fwd_core_train_6.pkl\n",
      "Pickled pickle/MLE_rev_core_train_6.pkl\n",
      "Pickled pickle/MLE_fwd_core_train_7.pkl\n",
      "Pickled pickle/MLE_rev_core_train_7.pkl\n",
      "Pickled pickle/MLE_fwd_core_train_8.pkl\n",
      "Pickled pickle/MLE_rev_core_train_8.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_core_train_8.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_core_train_8.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_2.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_2.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_3.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_3.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_4.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_4.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_5.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_5.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_6.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_6.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_7.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_7.pkl\n",
      "Pickled pickle/Laplace_fwd_core_train_8.pkl\n",
      "Pickled pickle/Laplace_rev_core_train_8.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_2.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_2.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_3.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_3.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_4.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_4.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_5.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_5.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_6.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_6.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_7.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_7.pkl\n",
      "Pickled pickle/Lidstone_fwd_core_train_8.pkl\n",
      "Pickled pickle/Lidstone_rev_core_train_8.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_2.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_2.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_3.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_3.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_4.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_4.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_5.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_5.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_6.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_6.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_7.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_7.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_core_train_8.pkl\n",
      "Pickled pickle/StupidBackoff_rev_core_train_8.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_core_train_8.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_core_train_8.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Instatiate the core train data trained models and pickle then\"\"\"\n",
    "for model_name in model_name_list:\n",
    "    for model_order in range(2, 9):\n",
    "        instantiate_and_save_model(model_name, model_order, 0.75,\"core_train\", core_tokenized_text, core_reverse_tokenized_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd9c7c",
   "metadata": {},
   "source": [
    "### All core data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa5bf3",
   "metadata": {},
   "source": [
    "### This is best model based on tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b575be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.lm.models.Lidstone object at 0x29c7b20e0>\n",
      "<nltk.lm.models.Lidstone object at 0x29c7b3130>\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Instantiate the Lidstone Septagram model on All data Core Indus data, fwd and rev\"\"\"\n",
    "k=7\n",
    "model_all_core_sep_fwd_lid = Lidstone(gamma, k) #Septagram model\n",
    "model_all_core_sep_rev_lid = Lidstone(gamma, k) #Septagram model\n",
    "\n",
    "\n",
    "core_fwd_data, core_padded_fwd_sents = padded_everygram_pipeline(k, all_core_tokenized_text)\n",
    "core_rev_data, core_padded_rev_sents = padded_everygram_pipeline(k, all_core_reverse_tokenized_text)\n",
    "\n",
    "\n",
    "print_data_details= False\n",
    "#If you iterate through this, the iterator is done with and model \n",
    "# fitting won't work subsequently\n",
    "# so set print_train_data_details= False before trying the actual model\n",
    "\n",
    "if(print_data_details):\n",
    "    for ngramlize_sent in core_fwd_data:\n",
    "        print(list(ngramlize_sent))\n",
    "        print()\n",
    "    print('#############')\n",
    "    list(core_padded_fwd_sents)\n",
    "    \n",
    "model_all_core_sep_fwd_lid.fit(core_fwd_data, core_padded_fwd_sents)\n",
    "model_all_core_sep_rev_lid.fit(core_rev_data, core_padded_rev_sents)\n",
    "    \n",
    "print(model_all_core_sep_fwd_lid)\n",
    "print(model_all_core_sep_rev_lid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d79ba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled Lidstone_fwd_all_core_train_7.pkl\n",
      "Pickled Lidstone_rev_all_core_train_7.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pickle Lidstone Septagram model for All Core Indus data language model fwd and reverse\"\"\"\n",
    "pickle.dump(model_all_core_sep_fwd_lid,open(\"pickle/Lidstone_fwd_all_core_train_7.pkl\", 'wb'))\n",
    "print(\"Pickled\",\"Lidstone_fwd_all_core_train_7.pkl\")\n",
    "\n",
    "pickle.dump(model_all_core_sep_rev_lid,open(\"pickle/Lidstone_rev_all_core_train_7.pkl\", 'wb'))\n",
    "print(\"Pickled\",\"Lidstone_rev_all_core_train_7.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
