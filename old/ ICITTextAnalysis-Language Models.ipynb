{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd828a7",
   "metadata": {},
   "source": [
    "# Indus Valley Script- Text Analysis for Decipherment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d860e",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "Dataset was created as a csv file from ICIT web site from raw html files of ICIT code for each for the Text\n",
    "Data labels were changes and a linearized copy of the original text was added\n",
    "\n",
    "### Input:\n",
    "Pickled data file from Input Data Processing\n",
    "\n",
    "### Output:\n",
    "Pickled model files from Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5b49",
   "metadata": {},
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install ipywidgets\n",
    "!pip install -U dill\n",
    "!pip3 install requests\n",
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa43bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import collections\n",
    "import random\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbae861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from collections import defaultdict\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e754e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8\n",
    "seed = 8\n",
    "CONST_INITIAL = \"Initial\"\n",
    "CONST_TERMINAL = \"Terminal\"\n",
    "CONST_MEDIAL = \"Medial\"\n",
    "CONST_NL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c87d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sign df: \n",
      "     id_sign sign_class set   graph       type        image variants  \\\n",
      "0         1        SIM  01  stroke     stroke  sign001.jpg        1   \n",
      "1         2        MKR  01  stroke     stroke  sign002.jpg        1   \n",
      "2         3        SIM  01  stroke     stroke  sign003.jpg        1   \n",
      "3         4        SIM  01  stroke     stroke  sign004.jpg        1   \n",
      "4         5        SIM  01  stroke     stroke  sign005.jpg        1   \n",
      "..      ...        ...  ..     ...        ...          ...      ...   \n",
      "704     952        CMX  71  animal  uncertain  sign952.jpg        1   \n",
      "705     953        CMX  71  animal       Pict  sign953.jpg        1   \n",
      "706     956        SIM  71       -    att.d.e  sign956.jpg        1   \n",
      "707     957        CMX  71       -  uncertain  sign957.jpg        1   \n",
      "708     958        CMX  71       -  uncertain  sign958.jpg        1   \n",
      "\n",
      "          function ligatur value frequency comment  \n",
      "0    NUM, ITM, SHN       -     -       227       -  \n",
      "1    ITM, SHN, EMS       -     -       865       -  \n",
      "2         NUM, SHN       -     -       260       -  \n",
      "3         NUM, SHN       -     -        99       -  \n",
      "4         NUM, SHN       -     -        49       -  \n",
      "..             ...     ...   ...       ...     ...  \n",
      "704            LFS       -     -         1       -  \n",
      "705            LFS       -     -         1       -  \n",
      "706            LOG       -     -         2       -  \n",
      "707            LOG       -     -         2       -  \n",
      "708            LFS       -     -         1       -  \n",
      "\n",
      "[709 rows x 12 columns]\n",
      "Updated original text df: \n",
      "      icit_id        site keywords text_class lines direction       text signs  \\\n",
      "0          1  Alamgirpur      NaN         SS     1       L/R  +410-017+     2   \n",
      "1          2  Alamgirpur      NaN         SS     1       L/R  +410-017+     2   \n",
      "2          3  Alamgirpur      NaN         SC     1       L/R  +405-017+     2   \n",
      "3          4   Allahdino      NaN         ??     1       NaN  +220-000+     1   \n",
      "4          5   Allahdino     Bull         UC     1       R/L  +740-235+     2   \n",
      "...      ...         ...      ...        ...   ...       ...        ...   ...   \n",
      "4994    4064     Harappa      NaN         UC     1       NaN      +000[     0   \n",
      "4995    4065     Harappa      NaN         VN     1       R/L  ]700-032[     2   \n",
      "4996    4065     Harappa      NaN         UC     1       R/L  ]000-000[     0   \n",
      "4997    4066     Harappa      NaN         UC     1       R/L  +368-000+     1   \n",
      "4998    4066     Harappa      NaN         VN     1       R/L  +700-033+     2   \n",
      "\n",
      "     complete    alignment  sign height text_images linearized_text  \\\n",
      "0           Y    Unordered      Unequal         NaN         410 017   \n",
      "1           Y          NaN          NaN         NaN         410 017   \n",
      "2           Y          NaN          NaN         NaN         405 017   \n",
      "3           N          NaN          NaN         NaN         220 000   \n",
      "4           Y          NaN          NaN         NaN         740 235   \n",
      "...       ...          ...          ...         ...             ...   \n",
      "4994        N  Indefinable  Indefinable         NaN            000[   \n",
      "4995        ?          NaN          NaN         NaN        700 032[   \n",
      "4996        N          NaN          NaN         NaN        000 000[   \n",
      "4997        N          NaN          NaN         NaN         368 000   \n",
      "4998        Y          NaN          NaN         NaN         700 033   \n",
      "\n",
      "     l_to_r_text r_to_l_text reversed_text  text_length  \n",
      "0        410 017     017 410       017 410          2.0  \n",
      "1        410 017     017 410       017 410          2.0  \n",
      "2        405 017     017 405       017 405          2.0  \n",
      "3        000 220     000 220       220 000          2.0  \n",
      "4        235 740     740 235       740 235          2.0  \n",
      "...          ...         ...           ...          ...  \n",
      "4994        000[        000[          000[          1.0  \n",
      "4995    032[ 700    700 032[      700 032[          3.0  \n",
      "4996    000[ 000    000 000[      000 000[          3.0  \n",
      "4997     000 368     368 000       368 000          2.0  \n",
      "4998     033 700     700 033       700 033          2.0  \n",
      "\n",
      "[4999 rows x 17 columns]\n",
      "Cleaned text df: \n",
      "      icit_id          site keywords text_class lines direction  \\\n",
      "0          1    Alamgirpur      NaN         SS     1       L/R   \n",
      "2          3    Alamgirpur      NaN         SC     1       L/R   \n",
      "4          5     Allahdino     Bull         UC     1       R/L   \n",
      "5          6     Allahdino    Bull1         SS     1       R/L   \n",
      "6          7     Allahdino    Bull1         SC     1       R/L   \n",
      "...      ...           ...      ...        ...   ...       ...   \n",
      "4953    4033  Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4956    4036  Mohenjo-daro      NaN         SS     1       R/L   \n",
      "4959    4039  Mohenjo-daro      NaN         SC     1       R/L   \n",
      "4967    4047  Mohenjo-daro      NaN         SC     1       R/L   \n",
      "4985    4059       Harappa      NaN         SP     1       R/L   \n",
      "\n",
      "                       text signs complete      alignment sign height  \\\n",
      "0                 +410-017+     2        Y      Unordered     Unequal   \n",
      "2                 +405-017+     2        Y            NaN         NaN   \n",
      "4                 +740-235+     2        Y            NaN         NaN   \n",
      "5             +740-390-590+     3        Y  Strong linear       Equal   \n",
      "6         +368-390-125-033+     4        Y         Linear       Equal   \n",
      "...                     ...   ...      ...            ...         ...   \n",
      "4953          +740-482-838+     3        Y            NaN         NaN   \n",
      "4956          +400-740-176+     3        Y            NaN         NaN   \n",
      "4959          +723-066-828+     3        Y         Linear       Equal   \n",
      "4967      +041-705-002-905+     4        Y            NaN         NaN   \n",
      "4985  +740-061-001-031-820+     5        Y            NaN         NaN   \n",
      "\n",
      "     text_images      linearized_text          l_to_r_text  \\\n",
      "0            NaN              410 017              410 017   \n",
      "2            NaN              405 017              405 017   \n",
      "4            NaN              740 235              235 740   \n",
      "5            NaN          740 390 590          590 390 740   \n",
      "6            NaN      368 390 125 033      033 125 390 368   \n",
      "...          ...                  ...                  ...   \n",
      "4953         NaN          740 482 838          838 482 740   \n",
      "4956         NaN          400 740 176          176 740 400   \n",
      "4959         NaN          723 066 828          828 066 723   \n",
      "4967         NaN      041 705 002 905      905 002 705 041   \n",
      "4985         NaN  740 061 001 031 820  820 031 001 061 740   \n",
      "\n",
      "              r_to_l_text        reversed_text  text_length  \n",
      "0                 017 410              017 410          2.0  \n",
      "2                 017 405              017 405          2.0  \n",
      "4                 740 235              740 235          2.0  \n",
      "5             740 390 590          740 390 590          4.0  \n",
      "6         368 390 125 033      368 390 125 033          5.0  \n",
      "...                   ...                  ...          ...  \n",
      "4953          740 482 838          740 482 838          4.0  \n",
      "4956          400 740 176          400 740 176          4.0  \n",
      "4959          723 066 828          723 066 828          4.0  \n",
      "4967      041 705 002 905      041 705 002 905          5.0  \n",
      "4985  740 061 001 031 820  740 061 001 031 820          6.0  \n",
      "\n",
      "[2223 rows x 17 columns]\n",
      "Unclear text df: \n",
      "      icit_id           site keywords text_class lines direction  \\\n",
      "12        13      Allahdino     Elep         UC     1       R/L   \n",
      "20        21       Bala-kot    Bull1         UC     1       R/L   \n",
      "22        23       Bala-kot      NaN         UC     1       R/L   \n",
      "61        62  Chanhujo-daro  Bull1:W         UC     1       R/L   \n",
      "81        82  Chanhujo-daro  Bull1:W         LP     1       R/L   \n",
      "...      ...            ...      ...        ...   ...       ...   \n",
      "4955    4035   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4958    4038   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4961    4041   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4965    4045   Mohenjo-daro      NaN         UC     1       R/L   \n",
      "4997    4066        Harappa      NaN         UC     1       R/L   \n",
      "\n",
      "                               text signs complete alignment sign height  \\\n",
      "12        +740-440-503-002-861-000+     5        N       NaN         NaN   \n",
      "20            +000-388-233-000-000+     2        N    Linear       Equal   \n",
      "22                    +750-790-000+     2        N       NaN         NaN   \n",
      "61    +000-100-240-220-032-002-861+     6        N    Linear       Equal   \n",
      "81    +740-032-840-002-296-530-000+     6        N    Linear     Adapted   \n",
      "...                             ...   ...      ...       ...         ...   \n",
      "4955                  +000-035-032+     2        N       NaN         NaN   \n",
      "4958                      +018-000+     1        N       NaN         NaN   \n",
      "4961          +400-740-760-000-000+     3        N       NaN         NaN   \n",
      "4965  +000-000-095-550-003-002-000+     4        N       NaN         NaN   \n",
      "4997                      +368-000+     1        N       NaN         NaN   \n",
      "\n",
      "     text_images              linearized_text                  l_to_r_text  \\\n",
      "12           NaN      740 440 503 002 861 000      000 861 002 503 440 740   \n",
      "20           NaN          000 388 233 000 000          000 000 233 388 000   \n",
      "22           NaN                  750 790 000                  000 790 750   \n",
      "61           NaN  000 100 240 220 032 002 861  861 002 032 220 240 100 000   \n",
      "81           NaN  740 032 840 002 296 530 000  000 530 296 002 840 032 740   \n",
      "...          ...                          ...                          ...   \n",
      "4955         NaN                  000 035 032                  032 035 000   \n",
      "4958         NaN                      018 000                      000 018   \n",
      "4961         NaN          400 740 760 000 000          000 000 760 740 400   \n",
      "4965         NaN  000 000 095 550 003 002 000  000 002 003 550 095 000 000   \n",
      "4997         NaN                      368 000                      000 368   \n",
      "\n",
      "                      r_to_l_text                reversed_text  text_length  \n",
      "12        740 440 503 002 861 000      740 440 503 002 861 000          8.0  \n",
      "20            000 388 233 000 000          000 388 233 000 000          6.0  \n",
      "22                    750 790 000                  750 790 000          4.0  \n",
      "61    000 100 240 220 032 002 861  000 100 240 220 032 002 861          9.0  \n",
      "81    740 032 840 002 296 530 000  740 032 840 002 296 530 000          9.0  \n",
      "...                           ...                          ...          ...  \n",
      "4955                  000 035 032                  000 035 032          4.0  \n",
      "4958                      018 000                      018 000          2.0  \n",
      "4961          400 740 760 000 000          400 740 760 000 000          6.0  \n",
      "4965  000 000 095 550 003 002 000  000 000 095 550 003 002 000          9.0  \n",
      "4997                      368 000                      368 000          2.0  \n",
      "\n",
      "[291 rows x 17 columns]\n",
      "Multi-line text df: \n",
      "      icit_id           site                          keywords text_class  \\\n",
      "69        70  Chanhujo-daro                          Bull1:II         2L   \n",
      "71        72  Chanhujo-daro                              Gaur         2L   \n",
      "72        73  Chanhujo-daro                             Bull1         2L   \n",
      "74        75  Chanhujo-daro                               NaN         2L   \n",
      "80        81  Chanhujo-daro                               NaN         2L   \n",
      "...      ...            ...                               ...        ...   \n",
      "4386    3538   Mohenjo-daro       Scene, Anth, Phyt, CompBull         2L   \n",
      "4402    3552   Mohenjo-daro                              Gaur         2L   \n",
      "4705    3814   Mohenjo-daro                               NaN         2L   \n",
      "4729    3836   Mohenjo-daro                               NaN         2L   \n",
      "4752    3857       Nausharo  Horned Composition of Anth, Tigr         2L   \n",
      "\n",
      "     lines direction                                       text signs  \\\n",
      "69       2       R/L                  +032-031/151-740-240-235+     6   \n",
      "71       2       R/L          +032-031/850-032-530-740-741-456+     8   \n",
      "72       2       R/L                      +032-031/740-791-713+     5   \n",
      "74       2       L/R                          +032/226-032-817+     4   \n",
      "80       2       R/L                      +740-636-240/002-817+     5   \n",
      "...    ...       ...                                        ...   ...   \n",
      "4386     2       NaN                  +621/090-740-231-560-534+     6   \n",
      "4402     2       R/L              +790/740-100-415-740-257-840+     7   \n",
      "4705     2       R/L                      +740-900-003/741-002+     5   \n",
      "4729     2       NaN                                  +840/790+     2   \n",
      "4752     2       BUS  +605-740-142-067/002-374-310-350-495-834+    10   \n",
      "\n",
      "     complete      alignment sign height text_images  \\\n",
      "69          Y         Linear     Unequal         NaN   \n",
      "71          Y      Unordered     Unequal         NaN   \n",
      "72          Y      Unordered     Unequal         NaN   \n",
      "74          Y         Linear       Equal         NaN   \n",
      "80          Y         Linear       Equal         NaN   \n",
      "...       ...            ...         ...         ...   \n",
      "4386        Y      Unordered     Unequal         NaN   \n",
      "4402        Y         Linear     Unequal         NaN   \n",
      "4705        Y            NaN         NaN         NaN   \n",
      "4729        Y            NaN         NaN         NaN   \n",
      "4752        Y  Partly linear     Unequal         NaN   \n",
      "\n",
      "                              linearized_text  \\\n",
      "69                    032 031 151 740 240 235   \n",
      "71            032 031 850 032 530 740 741 456   \n",
      "72                        032 031 740 791 713   \n",
      "74                            032 226 032 817   \n",
      "80                        740 636 240 002 817   \n",
      "...                                       ...   \n",
      "4386                  621 090 740 231 560 534   \n",
      "4402              790 740 100 415 740 257 840   \n",
      "4705                      740 900 003 741 002   \n",
      "4729                                  840 790   \n",
      "4752  605 740 142 067 002 374 310 350 495 834   \n",
      "\n",
      "                                  l_to_r_text  \\\n",
      "69                    235 240 740 151 031 032   \n",
      "71            456 741 740 530 032 850 031 032   \n",
      "72                        713 791 740 031 032   \n",
      "74                            032 226 032 817   \n",
      "80                        817 002 240 636 740   \n",
      "...                                       ...   \n",
      "4386                  534 560 231 740 090 621   \n",
      "4402              840 257 740 415 100 740 790   \n",
      "4705                      002 741 003 900 740   \n",
      "4729                                  790 840   \n",
      "4752  834 495 350 310 374 002 067 142 740 605   \n",
      "\n",
      "                                  r_to_l_text  \\\n",
      "69                    032 031 151 740 240 235   \n",
      "71            032 031 850 032 530 740 741 456   \n",
      "72                        032 031 740 791 713   \n",
      "74                            817 032 226 032   \n",
      "80                        740 636 240 002 817   \n",
      "...                                       ...   \n",
      "4386                  534 560 231 740 090 621   \n",
      "4402              790 740 100 415 740 257 840   \n",
      "4705                      740 900 003 741 002   \n",
      "4729                                  790 840   \n",
      "4752  834 495 350 310 374 002 067 142 740 605   \n",
      "\n",
      "                                reversed_text  text_length  \n",
      "69                    032 031 151 740 240 235          8.0  \n",
      "71            032 031 850 032 530 740 741 456         10.0  \n",
      "72                        032 031 740 791 713          6.0  \n",
      "74                            817 032 226 032          5.0  \n",
      "80                        740 636 240 002 817          6.0  \n",
      "...                                       ...          ...  \n",
      "4386                  621 090 740 231 560 534          8.0  \n",
      "4402              790 740 100 415 740 257 840          9.0  \n",
      "4705                      740 900 003 741 002          6.0  \n",
      "4729                                  840 790          2.0  \n",
      "4752  605 740 142 067 002 374 310 350 495 834         13.0  \n",
      "\n",
      "[77 rows x 17 columns]\n",
      "all df: \n",
      "               l_to_r_text direction          site\n",
      "0                 410 017       L/R    Alamgirpur\n",
      "1                 405 017       L/R    Alamgirpur\n",
      "2                 235 740       R/L     Allahdino\n",
      "3             590 390 740       R/L     Allahdino\n",
      "4         033 125 390 368       R/L     Allahdino\n",
      "...                   ...       ...           ...\n",
      "2218          838 482 740       R/L  Mohenjo-daro\n",
      "2219          176 740 400       R/L  Mohenjo-daro\n",
      "2220          828 066 723       R/L  Mohenjo-daro\n",
      "2221      905 002 705 041       R/L  Mohenjo-daro\n",
      "2222  820 031 001 061 740       R/L       Harappa\n",
      "\n",
      "[2223 rows x 3 columns]\n",
      "train df: \n",
      "                                             l_to_r_text direction  \\\n",
      "0                           817 002 013 840 176 100 740       R/L   \n",
      "1                                       125 384 700 875       R/L   \n",
      "2     140 919 520 360 861 002 235 231 705 033 032 87...       R/L   \n",
      "3                                               818 002       R/L   \n",
      "4                           032 590 741 350 460 798 740       R/L   \n",
      "...                                                 ...       ...   \n",
      "1995                                    240 100 740 400       R/L   \n",
      "1996                                    840 032 740 400       R/L   \n",
      "1997                                    370 002 550 527       R/L   \n",
      "1998                                    235 220 222 740       R/L   \n",
      "1999                            820 002 803 032 384 740       R/L   \n",
      "\n",
      "              site  \n",
      "0     Mohenjo-daro  \n",
      "1     Mohenjo-daro  \n",
      "2     Mohenjo-daro  \n",
      "3     Mohenjo-daro  \n",
      "4          Harappa  \n",
      "...            ...  \n",
      "1995       Harappa  \n",
      "1996       Harappa  \n",
      "1997  Mohenjo-daro  \n",
      "1998  Mohenjo-daro  \n",
      "1999  Mohenjo-daro  \n",
      "\n",
      "[2000 rows x 3 columns]\n",
      "test df: \n",
      "                      l_to_r_text direction          site\n",
      "0                        003 390       R/L  Mohenjo-daro\n",
      "1                    235 240 520       R/L       Harappa\n",
      "2    861 002 003 220 590 405 740       R/L       Harappa\n",
      "3                            820        NR       Harappa\n",
      "4    140 287 002 415 220 879 740       R/L  Mohenjo-daro\n",
      "..                           ...       ...           ...\n",
      "218              235 240 055 151       R/L  Mohenjo-daro\n",
      "219              235 176 740 400       R/L       Harappa\n",
      "220                      831 617       R/L  Mohenjo-daro\n",
      "221              235 240 904 740       R/L        Lothal\n",
      "222                  125 861 740       R/L  Mohenjo-daro\n",
      "\n",
      "[223 rows x 3 columns]\n",
      "all_rev df: \n",
      "             reversed_text          site\n",
      "0                 017 410    Alamgirpur\n",
      "1                 017 405    Alamgirpur\n",
      "2                 740 235     Allahdino\n",
      "3             740 390 590     Allahdino\n",
      "4         368 390 125 033     Allahdino\n",
      "...                   ...           ...\n",
      "2218          740 482 838  Mohenjo-daro\n",
      "2219          400 740 176  Mohenjo-daro\n",
      "2220          723 066 828  Mohenjo-daro\n",
      "2221      041 705 002 905  Mohenjo-daro\n",
      "2222  740 061 001 031 820       Harappa\n",
      "\n",
      "[2223 rows x 2 columns]\n",
      "train_rev df: \n",
      "                                           reversed_text          site\n",
      "0                           740 100 176 840 013 002 817  Mohenjo-daro\n",
      "1                                       875 700 384 125  Mohenjo-daro\n",
      "2     740 877 032 033 705 231 235 002 861 360 520 91...  Mohenjo-daro\n",
      "3                                               002 818  Mohenjo-daro\n",
      "4                           740 798 460 350 741 590 032       Harappa\n",
      "...                                                 ...           ...\n",
      "1995                                    400 740 100 240       Harappa\n",
      "1996                                    400 740 032 840       Harappa\n",
      "1997                                    527 550 002 370  Mohenjo-daro\n",
      "1998                                    740 222 220 235  Mohenjo-daro\n",
      "1999                            740 384 032 803 002 820  Mohenjo-daro\n",
      "\n",
      "[2000 rows x 2 columns]\n",
      "test rev df: \n",
      "                    reversed_text          site\n",
      "0                        390 003  Mohenjo-daro\n",
      "1                    520 240 235       Harappa\n",
      "2    740 405 590 220 003 002 861       Harappa\n",
      "3                            820       Harappa\n",
      "4    740 879 220 415 002 287 140  Mohenjo-daro\n",
      "..                           ...           ...\n",
      "218              151 055 240 235  Mohenjo-daro\n",
      "219              400 740 176 235       Harappa\n",
      "220                      617 831  Mohenjo-daro\n",
      "221              740 904 240 235        Lothal\n",
      "222                  740 861 125  Mohenjo-daro\n",
      "\n",
      "[223 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\" UnPickle the dataframes \"\"\"\n",
    "\n",
    "orig_sign_df = pd.read_pickle('pickle/orig_sign_df.pkl')\n",
    "print(\"Original sign df: \\n\", orig_sign_df)\n",
    "\n",
    "orig_df = pd.read_pickle('pickle/upd_orig_df.pkl')\n",
    "print(\"Updated original text df: \\n\", orig_df)\n",
    "\n",
    "df = pd.read_pickle('pickle/clean_df.pkl')\n",
    "print(\"Cleaned text df: \\n\", df )\n",
    "\n",
    "df_unclear = pd.read_pickle('pickle/unclear_df.pkl')\n",
    "print(\"Unclear text df: \\n\", df_unclear)\n",
    "\n",
    "df_multi_line = pd.read_pickle('pickle/multi_line_df.pkl')\n",
    "print(\"Multi-line text df: \\n\", df_multi_line)\n",
    "\n",
    "df_all_x = pd.read_pickle('pickle/all_x.pkl')\n",
    "df_all_y = pd.read_pickle('pickle/all_y.pkl')\n",
    "\n",
    "df_all_x_rev = pd.read_pickle('pickle/all_x_rev.pkl')\n",
    "df_all_y_rev = pd.read_pickle('pickle/all_y_rev.pkl')\n",
    "\n",
    "df_train_x = pd.read_pickle('pickle/train_x.pkl')\n",
    "df_train_y = pd.read_pickle('pickle/train_y.pkl')\n",
    "\n",
    "df_train_x_rev = pd.read_pickle('pickle/train_x_rev.pkl')\n",
    "df_train_y_rev = pd.read_pickle('pickle/train_y_rev.pkl')\n",
    "\n",
    "df_test_x = pd.read_pickle('pickle/test_x.pkl')\n",
    "df_test_y= pd.read_pickle('pickle/test_y.pkl')\n",
    "\n",
    "df_test_x_rev = pd.read_pickle('pickle/test_x_rev.pkl')\n",
    "df_test_y_rev = pd.read_pickle('pickle/test_y_rev.pkl')\n",
    "\n",
    "df_all = pd.read_pickle('pickle/all_df.pkl')\n",
    "print(\"all df: \\n\", df_all)\n",
    "\n",
    "df_train = pd.read_pickle('pickle/train_df.pkl')\n",
    "print(\"train df: \\n\", df_train)\n",
    "\n",
    "df_test = pd.read_pickle('pickle/test_df.pkl')\n",
    "print(\"test df: \\n\", df_test)\n",
    "\n",
    "df_all_rev = pd.read_pickle('pickle/all_rev_df.pkl')\n",
    "print(\"all_rev df: \\n\", df_all_rev)\n",
    "\n",
    "df_train_rev = pd.read_pickle('pickle/train_rev_df.pkl')\n",
    "print(\"train_rev df: \\n\", df_train_rev)\n",
    "\n",
    "df_test_rev = pd.read_pickle('pickle/test_rev_df.pkl')\n",
    "print(\"test rev df: \\n\", df_test_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f34d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_text(text):\n",
    "    # first split the string into chars\n",
    "    chars = text.split(' ')\n",
    "\n",
    "    # then reverse the split string list and join with a space\n",
    "    reversed_text = ' '.join(reversed(chars))\n",
    "    return reversed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f265a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ngram as list given a text (pass direction_of_string as \"R/L\" or \"L/R\")\n",
    "def get_ngrams_as_list(data,direction_of_string,num):\n",
    "    \n",
    "    if(direction_of_string==\"R/L\"):\n",
    "        # We need to convert R/L text to L/R to be able to get ngrams using nltk\n",
    "        data_string = reverse_text(data)\n",
    "    \n",
    "    else: data_string= data\n",
    "    \n",
    "    n_grams =  ngrams(nltk.word_tokenize(data_string), num)\n",
    "    return  [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4285844",
   "metadata": {},
   "source": [
    "## n-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da414ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tokenize the text\n",
    "\n",
    "If we need to generate ngrams from it from r to l text, ngrams would be in opposite direction, so\n",
    "use reversed text to generate tokenized_text (l to r) and regular text to generate reverse_tokenized_text (r to l)\n",
    "\"\"\"   \n",
    "\"\"\"For all data\"\"\"\n",
    "tokenized_text_all = list(df_all_x[df_all_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "reverse_tokenized_text_all = list(df_all_x_rev[df_all_x_rev.reversed_text!=''].reversed_text.apply(word_tokenize))\n",
    "\n",
    "\n",
    "\"\"\"For Train data\"\"\"\n",
    "tokenized_text = list(df_train_x[df_train_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "reverse_tokenized_text = list(df_train_x_rev[df_train_x_rev.reversed_text!=''].reversed_text.apply(word_tokenize))\n",
    "\n",
    "\n",
    "#print(\"tokenized_text:\",tokenized_text)\n",
    "#print(\"Rev tokenized_text:\",reverse_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba770861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess the tokenized text for n-grams language modeling\n",
    "Do this for all data an train data\n",
    "\"\"\"\n",
    "import array as arr\n",
    "model_name_list = [\"MLE\",\"KneserNeyInterpolated\", \"Laplace\", \"Lidstone\",\"StupidBackoff\", \"WittenBellInterpolated\"]\n",
    "\n",
    "all_data_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_unigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_unigram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_unigram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_unigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_unigram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_bigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_bigram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_bigram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_bigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_bigram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_trigram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_trigram  = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_trigram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_trigram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_trigram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_quadgram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_quadgram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_pentagram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_pentagram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_hexagram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_hexagram = [None,None, None, None, None,None]\n",
    "\n",
    "all_data_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "all_data_list_rev_septagram = [None,None, None, None, None,None]\n",
    "all_padded_sents_list_rev_septagram = [None,None, None, None, None,None]\n",
    "train_data_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_fwd_septagram = [None,None, None, None, None,None]\n",
    "train_data_list_rev_septagram = [None,None, None, None, None,None]\n",
    "padded_sents_list_rev_septagram = [None,None, None, None, None,None]\n",
    "\n",
    "train_data_rev_list = [None,None, None, None, None,None]\n",
    "padded_sents_rev_list = [None,None, None, None, None,None]\n",
    "\n",
    "\n",
    "for index in range (0,6):\n",
    "\n",
    "    all_data_list_fwd_unigram[index], all_padded_sents_list_fwd_unigram[index] = padded_everygram_pipeline(1, tokenized_text)\n",
    "    all_data_list_rev_unigram[index], all_padded_sents_list_rev_unigram[index] = padded_everygram_pipeline(1, reverse_tokenized_text)\n",
    "    train_data_list_fwd_unigram[index], padded_sents_list_fwd_unigram[index] = padded_everygram_pipeline(1, tokenized_text)\n",
    "    train_data_list_rev_unigram[index], padded_sents_list_rev_unigram[index] = padded_everygram_pipeline(1, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_bigram[index], all_padded_sents_list_fwd_bigram[index] = padded_everygram_pipeline(2, tokenized_text)\n",
    "    all_data_list_rev_bigram[index], all_padded_sents_list_rev_bigram[index] = padded_everygram_pipeline(2, reverse_tokenized_text)\n",
    "    train_data_list_fwd_bigram[index], padded_sents_list_fwd_bigram[index] = padded_everygram_pipeline(2, tokenized_text)\n",
    "    train_data_list_rev_bigram[index], padded_sents_list_rev_bigram[index] = padded_everygram_pipeline(2, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_trigram[index], all_padded_sents_list_fwd_trigram[index] = padded_everygram_pipeline(3, tokenized_text)\n",
    "    all_data_list_rev_trigram[index], all_padded_sents_list_rev_trigram[index] = padded_everygram_pipeline(3, reverse_tokenized_text)\n",
    "    train_data_list_fwd_trigram[index], padded_sents_list_fwd_trigram[index] = padded_everygram_pipeline(3, tokenized_text)\n",
    "    train_data_list_rev_trigram[index], padded_sents_list_rev_trigram[index] = padded_everygram_pipeline(3, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_quadgram[index], all_padded_sents_list_fwd_quadgram[index] = padded_everygram_pipeline(4, tokenized_text)\n",
    "    all_data_list_rev_quadgram[index], all_padded_sents_list_rev_quadgram[index] = padded_everygram_pipeline(4, reverse_tokenized_text)\n",
    "    train_data_list_fwd_quadgram[index], padded_sents_list_fwd_quadgram[index] = padded_everygram_pipeline(4, tokenized_text)\n",
    "    train_data_list_rev_quadgram[index], padded_sents_list_rev_quadgram[index] = padded_everygram_pipeline(4, reverse_tokenized_text)\n",
    "\n",
    "    all_data_list_fwd_pentagram[index], all_padded_sents_list_fwd_pentagram[index] = padded_everygram_pipeline(5, tokenized_text)\n",
    "    all_data_list_rev_pentagram[index], all_padded_sents_list_rev_pentagram[index] = padded_everygram_pipeline(5, reverse_tokenized_text)\n",
    "    train_data_list_fwd_pentagram[index], padded_sents_list_fwd_pentagram[index] = padded_everygram_pipeline(5, tokenized_text)\n",
    "    train_data_list_rev_pentagram[index], padded_sents_list_rev_pentagram[index] = padded_everygram_pipeline(5, reverse_tokenized_text)\n",
    "\n",
    "    all_data_list_fwd_hexagram[index], all_padded_sents_list_fwd_hexagram[index] = padded_everygram_pipeline(6, tokenized_text)\n",
    "    all_data_list_rev_hexagram[index], all_padded_sents_list_rev_hexagram[index] = padded_everygram_pipeline(6, reverse_tokenized_text)\n",
    "    train_data_list_fwd_hexagram[index], padded_sents_list_fwd_hexagram[index] = padded_everygram_pipeline(6, tokenized_text)\n",
    "    train_data_list_rev_hexagram[index], padded_sents_list_rev_hexagram[index] = padded_everygram_pipeline(6, reverse_tokenized_text)\n",
    "    \n",
    "    all_data_list_fwd_septagram[index], all_padded_sents_list_fwd_septagram[index] = padded_everygram_pipeline(7, tokenized_text)\n",
    "    all_data_list_rev_septagram[index], all_padded_sents_list_rev_septagram[index] = padded_everygram_pipeline(7, reverse_tokenized_text)\n",
    "    train_data_list_fwd_septagram[index], padded_sents_list_fwd_septagram[index] = padded_everygram_pipeline(7, tokenized_text)\n",
    "    train_data_list_rev_septagram[index], padded_sents_list_rev_septagram[index] = padded_everygram_pipeline(7, reverse_tokenized_text)\n",
    "\n",
    "    \n",
    "print_train_data_details= False\n",
    "#If you iterate through this, the iterator is done with and model \n",
    "# fitting won't work subsequently\n",
    "# so set print_train_data_details= False before trying the actual model\n",
    "\n",
    "# Example\n",
    "if(print_train_data_details):\n",
    "    for ngramlize_sent in train_data_list_fwd_quadgram[0]:\n",
    "        print(list(ngramlize_sent))\n",
    "        print()\n",
    "    print('#############')\n",
    "    list(padded_sents_list_fwd_quadgram[0])\n",
    "    \n",
    "if(print_train_data_details):\n",
    "    for ngramlize_sent in train_data_list_rev_quadgram[0]:\n",
    "        print(list(ngramlize_sent))\n",
    "        print()\n",
    "    print('#############')\n",
    "    list(padded_sents_list_rev_quadgram[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2276dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For All data: Unigram, Bigram, Trigram, Quadgram, Pentagram, Hexagram Models for both fwd text and reverse tex with the following\n",
    "models. Ignoring AbsoluteDiscountingInterpolated model\n",
    "\"\"\"\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import AbsoluteDiscountingInterpolated\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.models import Lidstone\n",
    "from nltk.lm.models import StupidBackoff\n",
    "from nltk.lm.models import WittenBellInterpolated\n",
    "\n",
    "gamma=0.75\n",
    "\n",
    "model_MLE_list_fwd_all = []\n",
    "model_KneserNeyInterpolated_list_fwd_all = []\n",
    "model_Laplace_list_fwd_all = []\n",
    "model_Lidstone_list_fwd_all = []\n",
    "model_StupidBackoff_list_fwd_all = []\n",
    "model_WittenBellInterpolated_list_fwd_all = []\n",
    "\n",
    "\n",
    "model_MLE_list_rev_all = []\n",
    "model_KneserNeyInterpolated_list_rev_all = []\n",
    "model_Laplace_list_rev_all = []\n",
    "model_Lidstone_list_rev_all = []\n",
    "model_StupidBackoff_list_rev_all = []\n",
    "model_WittenBellInterpolated_list_rev_all= []\n",
    "\n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_fwd_all.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_fwd_all.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_fwd_all.append(Laplace(index))\n",
    "    model_Lidstone_list_fwd_all.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_fwd_all.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_fwd_all.append(WittenBellInterpolated(index))\n",
    "    \n",
    "    \n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_rev_all.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_rev_all.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_rev_all.append(Laplace(index))\n",
    "    model_Lidstone_list_rev_all.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_rev_all.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_rev_all.append(WittenBellInterpolated(index))\n",
    "    \n",
    "models_list_fwd_unigram_all = [model_MLE_list_fwd_all[0] ,model_KneserNeyInterpolated_list_fwd_all[0] ,model_Laplace_list_fwd_all[0] , model_Lidstone_list_fwd_all[0] , model_StupidBackoff_list_fwd_all[0],model_WittenBellInterpolated_list_fwd_all[0]]\n",
    "models_list_rev_unigram_all = [model_MLE_list_rev_all[0] ,model_KneserNeyInterpolated_list_rev_all[0] ,model_Laplace_list_rev_all[0] , model_Lidstone_list_rev_all[0] , model_StupidBackoff_list_rev_all[0], model_WittenBellInterpolated_list_rev_all[0]]\n",
    "\n",
    "models_list_fwd_bigram_all = [model_MLE_list_fwd_all[1] ,model_KneserNeyInterpolated_list_fwd_all[1] ,model_Laplace_list_fwd_all[1] , model_Lidstone_list_fwd_all[1] , model_StupidBackoff_list_fwd_all[1],model_WittenBellInterpolated_list_fwd_all[1]]\n",
    "models_list_rev_bigram_all = [model_MLE_list_rev_all[1] ,model_KneserNeyInterpolated_list_rev_all[1] ,model_Laplace_list_rev_all[1] , model_Lidstone_list_rev_all[1] , model_StupidBackoff_list_rev_all[1], model_WittenBellInterpolated_list_rev_all[1]]\n",
    "\n",
    "models_list_fwd_trigram_all = [model_MLE_list_fwd_all[2] ,model_KneserNeyInterpolated_list_fwd_all[2] ,model_Laplace_list_fwd_all[2] , model_Lidstone_list_fwd_all[2] , model_StupidBackoff_list_fwd_all[2],model_WittenBellInterpolated_list_fwd_all[2]]\n",
    "models_list_rev_trigram_all = [model_MLE_list_rev_all[2] ,model_KneserNeyInterpolated_list_rev_all[2] ,model_Laplace_list_rev_all[2] , model_Lidstone_list_rev_all[2] , model_StupidBackoff_list_rev_all[2],model_WittenBellInterpolated_list_rev_all[2]]\n",
    "\n",
    "models_list_fwd_quadgram_all = [model_MLE_list_fwd_all[3] ,model_KneserNeyInterpolated_list_fwd_all[3] ,model_Laplace_list_fwd_all[3] , model_Lidstone_list_fwd_all[3] , model_StupidBackoff_list_fwd_all[3],model_WittenBellInterpolated_list_fwd_all[3]]\n",
    "models_list_rev_quadgram_all = [model_MLE_list_rev_all[3] ,model_KneserNeyInterpolated_list_rev_all[3] ,model_Laplace_list_rev_all[3] , model_Lidstone_list_rev_all[3] , model_StupidBackoff_list_rev_all[3],model_WittenBellInterpolated_list_rev_all[3]]\n",
    "\n",
    "models_list_fwd_pentagram_all = [model_MLE_list_fwd_all[4] ,model_KneserNeyInterpolated_list_fwd_all[4] ,model_Laplace_list_fwd_all[4] , model_Lidstone_list_fwd_all[4] , model_StupidBackoff_list_fwd_all[4],model_WittenBellInterpolated_list_fwd_all[4]]\n",
    "models_list_rev_pentagram_all = [model_MLE_list_rev_all[4] ,model_KneserNeyInterpolated_list_rev_all[4] ,model_Laplace_list_rev_all[4] , model_Lidstone_list_rev_all[4] , model_StupidBackoff_list_rev_all[4],model_WittenBellInterpolated_list_rev_all[4]]\n",
    "\n",
    "models_list_fwd_hexagram_all = [model_MLE_list_fwd_all[5] ,model_KneserNeyInterpolated_list_fwd_all[5] ,model_Laplace_list_fwd_all[5] , model_Lidstone_list_fwd_all[5] , model_StupidBackoff_list_fwd_all[5],model_WittenBellInterpolated_list_fwd_all[5]]\n",
    "models_list_rev_hexagram_all = [model_MLE_list_rev_all[5] ,model_KneserNeyInterpolated_list_rev_all[5] ,model_Laplace_list_rev_all[5] , model_Lidstone_list_rev_all[5] , model_StupidBackoff_list_rev_all[5],model_WittenBellInterpolated_list_rev_all[5]]\n",
    "\n",
    "models_list_fwd_septagram_all = [model_MLE_list_fwd_all[6] ,model_KneserNeyInterpolated_list_fwd_all[6] ,model_Laplace_list_fwd_all[6] , model_Lidstone_list_fwd_all[6] , model_StupidBackoff_list_fwd_all[6],model_WittenBellInterpolated_list_fwd_all[6]]\n",
    "models_list_rev_septagram_all = [model_MLE_list_rev_all[6] ,model_KneserNeyInterpolated_list_rev_all[6] ,model_Laplace_list_rev_all[6] , model_Lidstone_list_rev_all[6] , model_StupidBackoff_list_rev_all[6],model_WittenBellInterpolated_list_rev_all[6]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3da5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For Train data: Unigram, Bigram, Trigram, Quadgram, Pentagram, Hexagram Models for both fwd text and reverse tex with the following\n",
    "models. Ignoring AbsoluteDiscountingInterpolated model\n",
    "\"\"\"\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.lm.models import AbsoluteDiscountingInterpolated\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.models import Lidstone\n",
    "from nltk.lm.models import StupidBackoff\n",
    "from nltk.lm.models import WittenBellInterpolated\n",
    "\n",
    "gamma=0.75\n",
    "\n",
    "model_MLE_list_fwd = []\n",
    "model_KneserNeyInterpolated_list_fwd = []\n",
    "model_Laplace_list_fwd = []\n",
    "model_Lidstone_list_fwd = []\n",
    "model_StupidBackoff_list_fwd = []\n",
    "model_WittenBellInterpolated_list_fwd= []\n",
    "\n",
    "\n",
    "model_MLE_list_rev = []\n",
    "model_KneserNeyInterpolated_list_rev = []\n",
    "model_Laplace_list_rev = []\n",
    "model_Lidstone_list_rev = []\n",
    "model_StupidBackoff_list_rev = []\n",
    "model_WittenBellInterpolated_list_rev= []\n",
    "\n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_fwd.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_fwd.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_fwd.append(Laplace(index))\n",
    "    model_Lidstone_list_fwd.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_fwd.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_fwd.append(WittenBellInterpolated(index))\n",
    "    \n",
    "    \n",
    "for index in range(1, 8):\n",
    "    model_MLE_list_rev.append(MLE(index))\n",
    "    model_KneserNeyInterpolated_list_rev.append(KneserNeyInterpolated(index))\n",
    "    model_Laplace_list_rev.append(Laplace(index))\n",
    "    model_Lidstone_list_rev.append(Lidstone(gamma, index))\n",
    "    model_StupidBackoff_list_rev.append(StupidBackoff(index, index))\n",
    "    model_WittenBellInterpolated_list_rev.append(WittenBellInterpolated(index))\n",
    "    \n",
    "models_list_fwd_unigram = [model_MLE_list_fwd[0] ,model_KneserNeyInterpolated_list_fwd[0] ,model_Laplace_list_fwd[0] , model_Lidstone_list_fwd[0] , model_StupidBackoff_list_fwd[0],model_WittenBellInterpolated_list_fwd[0]]\n",
    "models_list_rev_unigram = [model_MLE_list_rev[0] ,model_KneserNeyInterpolated_list_rev[0] ,model_Laplace_list_rev[0] , model_Lidstone_list_rev[0] , model_StupidBackoff_list_rev[0], model_WittenBellInterpolated_list_rev[0]]\n",
    "\n",
    "models_list_fwd_bigram = [model_MLE_list_fwd[1] ,model_KneserNeyInterpolated_list_fwd[1] ,model_Laplace_list_fwd[1] , model_Lidstone_list_fwd[1] , model_StupidBackoff_list_fwd[1],model_WittenBellInterpolated_list_fwd[1]]\n",
    "models_list_rev_bigram = [model_MLE_list_rev[1] ,model_KneserNeyInterpolated_list_rev[1] ,model_Laplace_list_rev[1] , model_Lidstone_list_rev[1] , model_StupidBackoff_list_rev[1], model_WittenBellInterpolated_list_rev[1]]\n",
    "\n",
    "models_list_fwd_trigram = [model_MLE_list_fwd[2] ,model_KneserNeyInterpolated_list_fwd[2] ,model_Laplace_list_fwd[2] , model_Lidstone_list_fwd[2] , model_StupidBackoff_list_fwd[2],model_WittenBellInterpolated_list_fwd[2]]\n",
    "models_list_rev_trigram = [model_MLE_list_rev[2] ,model_KneserNeyInterpolated_list_rev[2] ,model_Laplace_list_rev[2] , model_Lidstone_list_rev[2] , model_StupidBackoff_list_rev[2],model_WittenBellInterpolated_list_rev[2]]\n",
    "\n",
    "models_list_fwd_quadgram = [model_MLE_list_fwd[3] ,model_KneserNeyInterpolated_list_fwd[3] ,model_Laplace_list_fwd[3] , model_Lidstone_list_fwd[3] , model_StupidBackoff_list_fwd[3],model_WittenBellInterpolated_list_fwd[3]]\n",
    "models_list_rev_quadgram = [model_MLE_list_rev[3] ,model_KneserNeyInterpolated_list_rev[3] ,model_Laplace_list_rev[3] , model_Lidstone_list_rev[3] , model_StupidBackoff_list_rev[3],model_WittenBellInterpolated_list_rev[3]]\n",
    "\n",
    "models_list_fwd_pentagram = [model_MLE_list_fwd[4] ,model_KneserNeyInterpolated_list_fwd[4] ,model_Laplace_list_fwd[4] , model_Lidstone_list_fwd[4] , model_StupidBackoff_list_fwd[4],model_WittenBellInterpolated_list_fwd[4]]\n",
    "models_list_rev_pentagram = [model_MLE_list_rev[4] ,model_KneserNeyInterpolated_list_rev[4] ,model_Laplace_list_rev[4] , model_Lidstone_list_rev[4] , model_StupidBackoff_list_rev[4],model_WittenBellInterpolated_list_rev[4]]\n",
    "\n",
    "models_list_fwd_hexagram = [model_MLE_list_fwd[5] ,model_KneserNeyInterpolated_list_fwd[5] ,model_Laplace_list_fwd[5] , model_Lidstone_list_fwd[5] , model_StupidBackoff_list_fwd[5],model_WittenBellInterpolated_list_fwd[5]]\n",
    "models_list_rev_hexagram = [model_MLE_list_rev[5] ,model_KneserNeyInterpolated_list_rev[5] ,model_Laplace_list_rev[5] , model_Lidstone_list_rev[5] , model_StupidBackoff_list_rev[5],model_WittenBellInterpolated_list_rev[5]]\n",
    "\n",
    "models_list_fwd_septagram = [model_MLE_list_fwd[6] ,model_KneserNeyInterpolated_list_fwd[6] ,model_Laplace_list_fwd[6] , model_Lidstone_list_fwd[6] , model_StupidBackoff_list_fwd[6],model_WittenBellInterpolated_list_fwd[6]]\n",
    "models_list_rev_septagram = [model_MLE_list_rev[6] ,model_KneserNeyInterpolated_list_rev[6] ,model_Laplace_list_rev[6] , model_Lidstone_list_rev[6] , model_StupidBackoff_list_rev[6],model_WittenBellInterpolated_list_rev[6]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02e83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_train_models(models_list,model_type, this_train_data_list,this_padded_sents_list):\n",
    "    for index in range (0,len(models_list)):\n",
    "        models_list[index].fit(this_train_data_list[index], this_padded_sents_list[index])\n",
    "        print(\"Fit:\",model_name_list[index], type, \"Order:\", models_list[index].order, models_list[index].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e96f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the models for All data\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the models for All data\")\n",
    "fit_and_train_models(models_list_fwd_unigram_all ,\"fwd\", all_data_list_fwd_unigram,all_padded_sents_list_fwd_unigram)\n",
    "fit_and_train_models(models_list_rev_unigram_all , \"rev\", all_data_list_rev_unigram,all_padded_sents_list_rev_unigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_bigram_all ,\"fwd\", all_data_list_fwd_bigram,all_padded_sents_list_fwd_bigram)\n",
    "fit_and_train_models(models_list_rev_bigram_all ,\"rev\", all_data_list_rev_bigram,all_padded_sents_list_rev_bigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_trigram_all ,\"fwd\", all_data_list_fwd_trigram,all_padded_sents_list_fwd_trigram)\n",
    "fit_and_train_models(models_list_rev_trigram_all , \"rev\",all_data_list_rev_trigram,all_padded_sents_list_rev_trigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_quadgram_all , \"fwd\",all_data_list_fwd_quadgram,all_padded_sents_list_fwd_quadgram)\n",
    "fit_and_train_models(models_list_rev_quadgram_all ,\"rev\", all_data_list_rev_quadgram,all_padded_sents_list_rev_quadgram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_pentagram_all ,\"fwd\", all_data_list_fwd_pentagram,all_padded_sents_list_fwd_pentagram)\n",
    "fit_and_train_models(models_list_rev_pentagram_all ,\"rev\", all_data_list_rev_pentagram,all_padded_sents_list_rev_pentagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_hexagram_all , \"fwd\",all_data_list_fwd_hexagram,all_padded_sents_list_fwd_hexagram)\n",
    "fit_and_train_models(models_list_rev_hexagram_all , \"rev\",all_data_list_rev_hexagram,all_padded_sents_list_rev_hexagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_septagram_all ,\"fwd\", all_data_list_fwd_septagram,all_padded_sents_list_fwd_septagram)\n",
    "fit_and_train_models(models_list_rev_septagram_all , \"rev\", all_data_list_rev_septagram,all_padded_sents_list_rev_septagram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c68df2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the models for Train data\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: MLE <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Laplace <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: Lidstone <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 1 <Vocabulary with cutoff=1 unk_label='<UNK>' and 581 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 2 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 3 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 4 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 5 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 6 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: MLE <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: KneserNeyInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: Laplace <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit: Lidstone <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: StupidBackoff <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n",
      "Fit: WittenBellInterpolated <class 'type'> Order: 7 <Vocabulary with cutoff=1 unk_label='<UNK>' and 583 items>\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the models for Train data\")\n",
    "\n",
    "fit_and_train_models(models_list_fwd_unigram ,\"fwd\", train_data_list_fwd_unigram,padded_sents_list_fwd_unigram)\n",
    "fit_and_train_models(models_list_rev_unigram , \"rev\", train_data_list_rev_unigram,padded_sents_list_rev_unigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_bigram ,\"fwd\", train_data_list_fwd_bigram,padded_sents_list_fwd_bigram)\n",
    "fit_and_train_models(models_list_rev_bigram ,\"rev\", train_data_list_rev_bigram,padded_sents_list_rev_bigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_trigram ,\"fwd\", train_data_list_fwd_trigram,padded_sents_list_fwd_trigram)\n",
    "fit_and_train_models(models_list_rev_trigram , \"rev\",train_data_list_rev_trigram,padded_sents_list_rev_trigram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_quadgram , \"fwd\",train_data_list_fwd_quadgram,padded_sents_list_fwd_quadgram)\n",
    "fit_and_train_models(models_list_rev_quadgram ,\"rev\", train_data_list_rev_quadgram,padded_sents_list_rev_quadgram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_pentagram ,\"fwd\", train_data_list_fwd_pentagram,padded_sents_list_fwd_pentagram)\n",
    "fit_and_train_models(models_list_rev_pentagram ,\"rev\", train_data_list_rev_pentagram,padded_sents_list_rev_pentagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_hexagram , \"fwd\",train_data_list_fwd_hexagram,padded_sents_list_fwd_hexagram)\n",
    "fit_and_train_models(models_list_rev_hexagram , \"rev\",train_data_list_rev_hexagram,padded_sents_list_rev_hexagram)\n",
    "\n",
    "fit_and_train_models(models_list_fwd_septagram ,\"fwd\", train_data_list_fwd_septagram,padded_sents_list_fwd_septagram)\n",
    "fit_and_train_models(models_list_rev_septagram , \"rev\", train_data_list_rev_septagram,padded_sents_list_rev_septagram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e22b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled pickle/MLE_fwd_all_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_1.pkl\n",
      "Pickled pickle/Laplace_fwd_all_1.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_1.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_1.pkl\n",
      "Pickled pickle/MLE_rev_all_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_1.pkl\n",
      "Pickled pickle/Laplace_rev_all_1.pkl\n",
      "Pickled pickle/Lidstone_rev_all_1.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_1.pkl\n",
      "Pickled pickle/MLE_fwd_all_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_2.pkl\n",
      "Pickled pickle/Laplace_fwd_all_2.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_2.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_2.pkl\n",
      "Pickled pickle/MLE_rev_all_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_2.pkl\n",
      "Pickled pickle/Laplace_rev_all_2.pkl\n",
      "Pickled pickle/Lidstone_rev_all_2.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_2.pkl\n",
      "Pickled pickle/MLE_fwd_all_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_3.pkl\n",
      "Pickled pickle/Laplace_fwd_all_3.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_3.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_3.pkl\n",
      "Pickled pickle/MLE_rev_all_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_3.pkl\n",
      "Pickled pickle/Laplace_rev_all_3.pkl\n",
      "Pickled pickle/Lidstone_rev_all_3.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_3.pkl\n",
      "Pickled pickle/MLE_fwd_all_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_4.pkl\n",
      "Pickled pickle/Laplace_fwd_all_4.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_4.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_4.pkl\n",
      "Pickled pickle/MLE_rev_all_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_4.pkl\n",
      "Pickled pickle/Laplace_rev_all_4.pkl\n",
      "Pickled pickle/Lidstone_rev_all_4.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_4.pkl\n",
      "Pickled pickle/MLE_fwd_all_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_5.pkl\n",
      "Pickled pickle/Laplace_fwd_all_5.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_5.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_5.pkl\n",
      "Pickled pickle/MLE_rev_all_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_5.pkl\n",
      "Pickled pickle/Laplace_rev_all_5.pkl\n",
      "Pickled pickle/Lidstone_rev_all_5.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_5.pkl\n",
      "Pickled pickle/MLE_fwd_all_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_6.pkl\n",
      "Pickled pickle/Laplace_fwd_all_6.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_6.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_6.pkl\n",
      "Pickled pickle/MLE_rev_all_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_6.pkl\n",
      "Pickled pickle/Laplace_rev_all_6.pkl\n",
      "Pickled pickle/Lidstone_rev_all_6.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_6.pkl\n",
      "Pickled pickle/MLE_fwd_all_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_all_7.pkl\n",
      "Pickled pickle/Laplace_fwd_all_7.pkl\n",
      "Pickled pickle/Lidstone_fwd_all_7.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_all_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_all_7.pkl\n",
      "Pickled pickle/MLE_rev_all_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_all_7.pkl\n",
      "Pickled pickle/Laplace_rev_all_7.pkl\n",
      "Pickled pickle/Lidstone_rev_all_7.pkl\n",
      "Pickled pickle/StupidBackoff_rev_all_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_all_7.pkl\n",
      "Pickled pickle/MLE_fwd_train_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_1.pkl\n",
      "Pickled pickle/Laplace_fwd_train_1.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_1.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_1.pkl\n",
      "Pickled pickle/MLE_rev_train_1.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_1.pkl\n",
      "Pickled pickle/Laplace_rev_train_1.pkl\n",
      "Pickled pickle/Lidstone_rev_train_1.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_1.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_1.pkl\n",
      "Pickled pickle/MLE_fwd_train_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_2.pkl\n",
      "Pickled pickle/Laplace_fwd_train_2.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_2.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_2.pkl\n",
      "Pickled pickle/MLE_rev_train_2.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_2.pkl\n",
      "Pickled pickle/Laplace_rev_train_2.pkl\n",
      "Pickled pickle/Lidstone_rev_train_2.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_2.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_2.pkl\n",
      "Pickled pickle/MLE_fwd_train_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_3.pkl\n",
      "Pickled pickle/Laplace_fwd_train_3.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_3.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_3.pkl\n",
      "Pickled pickle/MLE_rev_train_3.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_3.pkl\n",
      "Pickled pickle/Laplace_rev_train_3.pkl\n",
      "Pickled pickle/Lidstone_rev_train_3.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_3.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_3.pkl\n",
      "Pickled pickle/MLE_fwd_train_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_4.pkl\n",
      "Pickled pickle/Laplace_fwd_train_4.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_4.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_4.pkl\n",
      "Pickled pickle/MLE_rev_train_4.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_4.pkl\n",
      "Pickled pickle/Laplace_rev_train_4.pkl\n",
      "Pickled pickle/Lidstone_rev_train_4.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_4.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_4.pkl\n",
      "Pickled pickle/MLE_fwd_train_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_5.pkl\n",
      "Pickled pickle/Laplace_fwd_train_5.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_5.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_5.pkl\n",
      "Pickled pickle/MLE_rev_train_5.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_5.pkl\n",
      "Pickled pickle/Laplace_rev_train_5.pkl\n",
      "Pickled pickle/Lidstone_rev_train_5.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_5.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_5.pkl\n",
      "Pickled pickle/MLE_fwd_train_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_6.pkl\n",
      "Pickled pickle/Laplace_fwd_train_6.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_6.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_6.pkl\n",
      "Pickled pickle/MLE_rev_train_6.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_6.pkl\n",
      "Pickled pickle/Laplace_rev_train_6.pkl\n",
      "Pickled pickle/Lidstone_rev_train_6.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_6.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_6.pkl\n",
      "Pickled pickle/MLE_fwd_train_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_fwd_train_7.pkl\n",
      "Pickled pickle/Laplace_fwd_train_7.pkl\n",
      "Pickled pickle/Lidstone_fwd_train_7.pkl\n",
      "Pickled pickle/StupidBackoff_fwd_train_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_fwd_train_7.pkl\n",
      "Pickled pickle/MLE_rev_train_7.pkl\n",
      "Pickled pickle/KneserNeyInterpolated_rev_train_7.pkl\n",
      "Pickled pickle/Laplace_rev_train_7.pkl\n",
      "Pickled pickle/Lidstone_rev_train_7.pkl\n",
      "Pickled pickle/StupidBackoff_rev_train_7.pkl\n",
      "Pickled pickle/WittenBellInterpolated_rev_train_7.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pickle all the trained Language Models\"\"\"\n",
    "def pickle_models(models_list, model_type, data_type):\n",
    "    for index in range (0,len(models_list)):\n",
    "        file_name = \"pickle/\" + model_name_list[index]+ \"_\" + model_type + \"_\" + data_type + \"_\" + str(models_list[index].order) +\".pkl\"\n",
    "        pickle.dump(models_list[index],open(file_name, 'wb'))\n",
    "        print(\"Pickled\",file_name)\n",
    "\n",
    "pickle_models(models_list_fwd_unigram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_unigram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_bigram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_bigram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_trigram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_trigram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_quadgram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_quadgram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_pentagram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_pentagram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_hexagram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_hexagram_all,\"rev\",\"all\")\n",
    "\n",
    "pickle_models(models_list_fwd_septagram_all,\"fwd\",\"all\")\n",
    "pickle_models(models_list_rev_septagram_all,\"rev\",\"all\")\n",
    "\n",
    "\n",
    "pickle_models(models_list_fwd_unigram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_unigram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_bigram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_bigram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_trigram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_trigram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_quadgram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_quadgram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_pentagram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_pentagram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_hexagram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_hexagram,\"rev\",\"train\")\n",
    "\n",
    "pickle_models(models_list_fwd_septagram,\"fwd\",\"train\")\n",
    "pickle_models(models_list_rev_septagram,\"rev\",\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730f194",
   "metadata": {},
   "source": [
    "## Initial Terminal Character Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff608753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 437 items>\n",
      "<NgramCounter with 2 ngram orders and 13125 ngrams>\n",
      "231\n",
      "817\n"
     ]
    }
   ],
   "source": [
    "# Build Model for relationship between Initial and Terminal characters\n",
    "# This can be a bigram model. Pick a reasonably good model\n",
    "# Remove all characters other than initial and terminal and then tokenize\n",
    "tokenized_text_temp = list(df_train_x[df_train_x.l_to_r_text!=''].l_to_r_text.apply(word_tokenize))\n",
    "\n",
    "#print(tokenized_text_temp)\n",
    "      \n",
    "tokenized_text_it = []\n",
    "for i in range(len(tokenized_text_temp)):\n",
    "    \n",
    "    l= tokenized_text_temp[i]\n",
    "    #single character text, ignore it\n",
    "    if(len(l)>1):\n",
    "        del l[1:len(l)-1]\n",
    "        l[0],l[1] = l[1], l[0]  #swap\n",
    "        tokenized_text_it.append(l)\n",
    "        \n",
    "#print(tokenized_text_it)\n",
    "\n",
    "\"\"\" Instantiate the model\"\"\"\n",
    "k=2\n",
    "model_it_bigram_kn = KneserNeyInterpolated(k) #Bigram model\n",
    "train_data_it, padded_sents_it = padded_everygram_pipeline(k, tokenized_text_it)\n",
    "\n",
    "\n",
    "print_train_data_details= False\n",
    "#If you iterate through this, the iterator is done with and model \n",
    "# fitting won't work subsequently\n",
    "# so set print_train_data_details= False before trying the actual model\n",
    "\n",
    "if(print_train_data_details):\n",
    "    for ngramlize_sent in train_data_it:\n",
    "        print(list(ngramlize_sent))\n",
    "        print()\n",
    "    print('#############')\n",
    "    list(padded_sents_it)\n",
    "    \n",
    "model_it_bigram_kn.fit(train_data_it, padded_sents_it)\n",
    "    \n",
    "print(model_it_bigram_kn.vocab)\n",
    "print(model_it_bigram_kn.counts)\n",
    "print(model_it_bigram_kn.generate(1, ['804'], 8))\n",
    "print(model_it_bigram_kn.generate(1, ['621'], 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ecebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled KneserNeyInterpolated_it_2.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pickle this model\"\"\"\n",
    "pickle.dump(model_it_bigram_kn,open(\"pickle/KneserNeyInterpolated_it_2.pkl\", 'wb'))\n",
    "print(\"Pickled\",\"KneserNeyInterpolated_it_2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
